"""
Risk Score Finalization - Uniform risk score computation with intel fusion.

This module provides centralized risk score calculation and finalization
to ensure consistent risk assessment across all investigation components.
Includes external intelligence fusion and sanity checking.
"""

import json
from typing import Dict, Any, Optional
from app.service.agent.orchestration.metrics.safe import safe_div, fmt_num, safe_mean
from app.service.agent.orchestration.metrics.network import compute_network_metrics
from app.service.intel.normalize import normalize_abuseipdb, abuseipdb_risk_component, normalize_virustotal, resolve_provider_conflicts
from app.service.risk.fusion import fuse_network_risk
from app.service.text.clean import clean_llm_analysis_sections
from .policy import apply_confirmed_fraud_floor, fuse_risk, derive_confirmed_fraud, prepublish_validate, isolate_llm_narrative
from app.service.logging import get_bridge_logger

logger = get_bridge_logger(__name__)


def compute_final_risk(state: Dict[str, Any]) -> Optional[float]:
    """
    Calculate final risk score using risk agent calculation method.
    
    CRITICAL: All graph types (hybrid, parallel, etc.) now use the same risk agent calculation.
    If LLM risk score exists, it's blended 50/50 with the risk agent calculation.
    
    Args:
        state: Investigation state containing domain findings
        
    Returns:
        Final risk score (0.0-1.0) or None if calculation fails
    """
    from app.service.agent.orchestration.domain_agents.risk_agent import _calculate_real_risk_score
    
    # Extract domain findings and facts
    domain_findings = state.get("domain_findings", {})
    facts = state.get("facts", {})
    
    if not domain_findings:
        logger.error("‚ùå No domain findings available for risk calculation")
        return None
    
    # CRITICAL DEBUG: Log what we actually receive
    logger.info(f"üîç compute_final_risk: domain_findings type={type(domain_findings)}, keys={list(domain_findings.keys()) if isinstance(domain_findings, dict) else 'NOT A DICT'}")
    if isinstance(domain_findings, dict):
        for domain_name in ["logs", "network", "device", "location", "authentication", "merchant"]:
            if domain_name in domain_findings:
                risk_score = domain_findings[domain_name].get("risk_score") if isinstance(domain_findings[domain_name], dict) else "NOT A DICT"
                logger.info(f"   {domain_name}: risk_score={risk_score}")
    
    # Use risk agent calculation method (PRIMARY METHOD)
    try:
        risk_agent_score = _calculate_real_risk_score(domain_findings, facts)
        logger.info(f"üìä Risk agent calculated score: {risk_agent_score:.4f}")
    except Exception as e:
        logger.error(f"‚ùå Risk agent calculation failed: {e}", exc_info=True)
        return None
    
    # Check if LLM risk score exists - if so, blend 50/50
    llm_risk_score = state.get("llm_risk_score")
    if llm_risk_score is not None:
        try:
            llm_score_float = float(llm_risk_score)
            if 0 <= llm_score_float <= 1.0:
                # Blend 50% LLM risk score + 50% risk agent score
                blended_score = (0.5 * llm_score_float) + (0.5 * risk_agent_score)
                logger.info(f"üß† LLM risk score found ({llm_score_float:.4f}), blending 50/50 with risk agent score ({risk_agent_score:.4f}) = {blended_score:.4f}")
                final_risk = min(1.0, max(0.0, blended_score))
                return final_risk
            else:
                logger.warning(f"‚ö†Ô∏è LLM risk score out of range ({llm_score_float}), using risk agent score only")
        except (ValueError, TypeError) as e:
            logger.warning(f"‚ö†Ô∏è Invalid LLM risk score format: {llm_risk_score}, using risk agent score only")
    
    # No LLM risk score or invalid - use risk agent score directly
    return risk_agent_score


def finalize_risk(state: Dict[str, Any]) -> None:
    """
    Finalize risk score in state using uniform computation with intel fusion.
    
    This enhanced finalization:
    1. Ensures timing is set (prevent duration errors)
    2. Fixes network metrics (unique IP counts)  
    3. Applies external intelligence fusion (AbuseIPDB, etc.)
    4. Cleans duplicate boilerplate in LLM analysis sections
    5. Performs sanity checks and assertions
    6. Ensures consistent risk scoring
    
    Args:
        state: Investigation state to update with final risk score
    """
    # FIRST: Ensure duration is set to prevent assertion failures
    _ensure_duration_for_finalization(state)
    
    # CRITICAL FIX: Isolate LLM narrative from engine math
    isolate_llm_narrative(state)
    
    # Fix network metrics for IP investigations
    compute_network_metrics(state)
    
    # Apply external intelligence fusion
    _apply_intel_fusion(state)
    
    # Clean LLM analysis sections to remove boilerplate
    _clean_llm_analysis_sections(state)
    
    # Deduplicate prose across all domain findings
    _deduplicate_cross_domain_prose(state)
    
    # Apply confirmed fraud floor to relevant domains
    _apply_confirmed_fraud_floors(state)
    
    # Compute evidence strength based on collected evidence
    _compute_evidence_strength(state)
    
    # Apply decisive evidence policy for confidence floors
    _apply_decisive_evidence_policy(state)
    
    # CRITICAL: Apply evidence gate before finalizing
    _apply_evidence_gate(state)
    
    # Compute final risk score
    final_risk = compute_final_risk(state)
    
    # CRITICAL FIX: Handle None from evidence gating
    if final_risk is None:
        logger.info("üö´ EVIDENCE GATING: Final risk blocked by aggregator - investigation needs more evidence")
        state["risk_score"] = None
        state["investigation_status"] = "insufficient_evidence"
        gating_reason = state.get("gating_reason", "Insufficient corroborating evidence")
        state["recommended_next_actions"] = [f"Investigation blocked: {gating_reason}"]
        return  # Exit early - no numeric risk to publish
    
    # CRITICAL PATCH E: Apply anti-flap guard to prevent wild swings
    entity_id = state.get("entity_id", "unknown")
    flap_result = _apply_anti_flap_guard(entity_id, final_risk, state)
    
    # Use flap-adjusted risk score
    final_risk = flap_result["adjusted_risk"]
    
    # CRITICAL: No fraud indicators can be used during investigation to prevent data leakage
    # All fraud indicator columns (IS_FRAUD_TX, COUNT_DISPUTES, COUNT_FRAUD_ALERTS, etc.) are excluded
    # Confirmed fraud detection must be based on behavioral patterns only, not fraud outcomes
    confirmed_fraud_detected = False
    
    # CRITICAL FIX: Hard evidence gating before publishing numeric risk
    validation_result = prepublish_validate(state)
    state["validation_result"] = validation_result
    
    # BYPASS EVIDENCE GATE for confirmed fraud cases (ground truth overrides evidence requirements)
    if confirmed_fraud_detected:
        logger.info("üö® CONFIRMED FRAUD BYPASS: Skipping evidence gate due to ground truth fraud confirmation")
        pass  # Skip evidence gate blocking logic
    elif not validation_result["can_publish_numeric_risk"]:
        # Block numeric risk publication - mark as needs more evidence
        state["risk_score"] = None  # No numeric risk published
        state["investigation_status"] = validation_result["status"]
        state["recommended_next_actions"] = validation_result["recommended_actions"]
        logger.warning(f"üö´ EVIDENCE GATE: Blocked numeric risk publication - {validation_result['status']}")
        return  # Exit early for blocked cases
    
    # Continue with risk finalization (for both confirmed fraud and evidence-passing cases)
    # CRITICAL: Apply confirmed fraud risk floor (ground truth adjudication)
    if confirmed_fraud_detected:
        original_risk_for_floor = final_risk
        fraud_floor = 0.60
        if final_risk is None or final_risk < fraud_floor:
            final_risk = fraud_floor
            logger.info(f"üö® CONFIRMED FRAUD FLOOR: Risk elevated from {fmt_num(original_risk_for_floor, 3)} to {fmt_num(final_risk, 3)} due to ground truth")
    
    # Apply risk cap for discordant signals (but respect fraud floor)
    if validation_result.get("discordant_signals", False):
        max_allowed = validation_result.get("max_allowed_risk", 0.4)
        # Don't cap below confirmed fraud floor
        if state.get("confirmed_fraud_present") and max_allowed < 0.60:
            max_allowed = 0.60
            logger.info("üõ°Ô∏è FRAUD FLOOR PROTECTION: Discordance cap raised to respect confirmed fraud floor")
        
        if final_risk > max_allowed:
            original_risk = final_risk
            final_risk = max_allowed
            logger.warning(f"üîí DISCORDANCE CAP: Risk capped from {fmt_num(original_risk, 3)} to {fmt_num(final_risk, 3)} due to discordant signals")
        state["discordant_signals"] = True
        state["investigation_status"] = "discordant_signals"
    else:
        state["investigation_status"] = "completed_with_numeric_risk"
    
    # CRITICAL: Run comprehensive linting checks before publishing risk
    from app.service.agent.orchestration.domain.linter import lint_investigation, assert_investigation_safety
    from app.service.agent.orchestration.domain.domain_result import DomainResult
    
    # Convert state domain findings to DomainResult objects for linting
    domain_findings = state.get("domain_findings", {})
    domains_for_linting = []
    
    for domain_name in ["logs", "network", "device", "location", "authentication"]:
        domain_data = domain_findings.get(domain_name, {})
        if isinstance(domain_data, dict):
            domain_result = DomainResult(
                name=domain_name,
                score=domain_data.get("risk_score"),
                status="OK" if domain_data.get("risk_score") is not None else "INSUFFICIENT_EVIDENCE",
                signals=domain_data.get("evidence", [])[:3],  # First 3 evidence items as signals
                confidence=domain_data.get("confidence", 0.35),
                narrative=domain_data.get("summary", f"Domain {domain_name} analysis")
            )
            domains_for_linting.append(domain_result)
    
    # CRITICAL FIX: Set state.risk_score BEFORE linting to prevent mismatch errors
    # The linter checks state.risk_score == final_risk, so we need to set it first
    # Only set if it's not already set or if it's different (to avoid overwriting intentionally)
    current_state_risk = state.get("risk_score")
    if current_state_risk != final_risk:
        # Temporarily set for linting check (will be set again after linting for clarity)
        state["risk_score"] = final_risk
        if current_state_risk is not None:
            logger.debug(f"üîÑ Updated state.risk_score from {fmt_num(current_state_risk, 3)} to {fmt_num(final_risk, 3)} before linting")
    
    # Run comprehensive linting checks
    lint_errors = lint_investigation(domains_for_linting, final_risk, state)
    
    if lint_errors:
        logger.warning(f"üö® LINTING VIOLATIONS DETECTED: {len(lint_errors)} issues before publishing risk")
        for i, error in enumerate(lint_errors[:5], 1):  # Log first 5 errors
            logger.warning(f"   LINT ERROR {i}: {error}")
        
        # Store linting results in state for debugging
        state["linting_errors"] = lint_errors
        state["linting_passed"] = False
    else:
        logger.info("‚úÖ All linting checks passed before risk publication")
        state["linting_passed"] = True
    
    # Run safety assertions (critical violations will raise AssertionError)
    try:
        assert_investigation_safety(domains_for_linting, final_risk, state)
        logger.debug("‚úÖ Investigation safety assertions passed")
    except AssertionError as e:
        logger.error(f"üö® CRITICAL SAFETY VIOLATION: {e}")
        # Log critical violation but allow investigation to continue
        state["safety_violation"] = str(e)
        state["investigation_warnings"] = state.get("investigation_warnings", [])
        state["investigation_warnings"].append(f"Safety violation: {e}")

    # Publish final risk (potentially adjusted by floor and cap)
    # Note: Already set before linting, but ensure it's set here for clarity and consistency
    state["risk_score"] = final_risk
    # CRITICAL: Also set overall_risk_score so it's available in progress_json
    # This ensures investigations have overall_risk_score for comparison metrics
    state["overall_risk_score"] = final_risk
    logger.debug(f"‚úÖ Risk finalization complete - numeric risk published: {fmt_num(final_risk, 3)}")
    
    # Generate severity-appropriate action plan based on final risk
    _generate_action_plan(state, final_risk)
    
    # Perform sanity checks
    _assert_sanity_checks(state, final_risk)
    
    logger.debug(f"Risk finalization complete: {fmt_num(final_risk, 3)}")


def get_risk_breakdown(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Get detailed risk score breakdown by domain.
    
    Args:
        state: Investigation state containing domain findings
        
    Returns:
        Risk breakdown with domain contributions
    """
    weights = {
        "network": 0.25,
        "logs": 0.25, 
        "device": 0.20,
        "location": 0.15,
        "authentication": 0.15
    }
    
    domain_findings = state.get("domain_findings", {})
    breakdown = {
        "final_risk_score": compute_final_risk(state),
        "domain_contributions": {},
        "total_weight_used": 0.0,
        "missing_domains": []
    }
    
    total_weight = 0.0
    for domain, weight in weights.items():
        domain_data = domain_findings.get(domain, {})
        risk_score = domain_data.get("risk_score")
        
        if isinstance(risk_score, (int, float)) and risk_score >= 0:
            contribution = weight * risk_score
            breakdown["domain_contributions"][domain] = {
                "risk_score": risk_score,
                "weight": weight,
                "contribution": round(contribution, 3)
            }
            total_weight += weight
        else:
            breakdown["missing_domains"].append(domain)
    
    breakdown["total_weight_used"] = total_weight
    return breakdown


def validate_risk_consistency(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate risk score consistency across the investigation.
    
    Args:
        state: Investigation state to validate
        
    Returns:
        Validation results with any inconsistencies found
    """
    final_risk = state.get("risk_score", 0.0)
    computed_risk = compute_final_risk(state)
    
    validation = {
        "is_consistent": abs(final_risk - computed_risk) < 0.01,
        "state_risk_score": final_risk,
        "computed_risk_score": computed_risk,
        "difference": round(abs(final_risk - computed_risk), 3),
        "issues": []
    }
    
    if not validation["is_consistent"]:
        validation["issues"].append(f"Risk score mismatch: state={final_risk}, computed={computed_risk}")
    
    # Check domain risk scores are valid
    domain_findings = state.get("domain_findings", {})
    for domain, data in domain_findings.items():
        risk_score = data.get("risk_score")
        if risk_score is not None:
            if not isinstance(risk_score, (int, float)):
                validation["issues"].append(f"Domain {domain} has invalid risk_score type: {type(risk_score)}")
            elif risk_score < 0 or risk_score > 1:
                validation["issues"].append(f"Domain {domain} risk_score out of range: {risk_score}")
    
    return validation


def _apply_intel_fusion(state: Dict[str, Any]) -> None:
    """
    Apply external intelligence fusion to network domain risk scores.
    
    Normalizes AbuseIPDB data and fuses with internal transaction risk.
    For confirmed fraud cases, preserves LLM analysis score as canonical.
    """
    try:
        domain_findings = state.get("domain_findings", {})
        network_findings = domain_findings.get("network", {})
        
        if not network_findings:
            return
        
        # Check if this is a confirmed fraud case - preserve LLM assessment
        llm_analysis = network_findings.get("llm_analysis", {})
        if llm_analysis:
            risk_factors = llm_analysis.get("risk_factors", "")
            reasoning = llm_analysis.get("reasoning", "")
            combined_text = (str(risk_factors) + " " + str(reasoning)).lower()
            
            if "confirmed fraud" in combined_text:
                # Preserve LLM analysis score - no fusion for confirmed fraud
                llm_risk = llm_analysis.get("risk_score")
                if llm_risk is not None:
                    # C2 FIX: Defensive normalization for LLM scores
                    if isinstance(llm_risk, (int, float)) and llm_risk > 1.0:
                        original_llm_risk = llm_risk
                        llm_risk = min(llm_risk / 100.0, 1.0)
                        logger.info(f"C2 FIX: Normalized confirmed fraud LLM score from {original_llm_risk} to {fmt_num(llm_risk, 3)}")

                    network_findings["risk_score"] = llm_risk
                    evidence = network_findings.setdefault("evidence", [])
                    evidence.append(f"Preserved LLM confirmed fraud assessment: {fmt_num(llm_risk, 3)} (no external fusion applied)")
                    logger.info(f"Confirmed fraud detected: preserving LLM risk score {fmt_num(llm_risk, 3)} (skipped intel fusion)")
                    return
        
        # CRITICAL FIX: Collect all threat intelligence providers and resolve conflicts
        tool_results = state.get("tool_results", {})
        provider_data = {}
        
        # Check for AbuseIPDB results
        abuse_result = tool_results.get("abuseipdb_ip_reputation")
        if abuse_result:
            try:
                parsed_abuse = json.loads(abuse_result) if isinstance(abuse_result, str) else abuse_result
                provider_data["abuseipdb"] = normalize_abuseipdb(parsed_abuse)
            except Exception as e:
                logger.warning(f"Failed to parse AbuseIPDB result: {e}")
        
        # Check for VirusTotal results
        vt_result = tool_results.get("virustotal_ip_analysis") or tool_results.get("virustotal_analysis")
        if vt_result:
            try:
                parsed_vt = json.loads(vt_result) if isinstance(vt_result, str) else vt_result
                provider_data["virustotal"] = normalize_virustotal(parsed_vt)
            except Exception as e:
                logger.warning(f"Failed to parse VirusTotal result: {e}")
        
        if provider_data:
            # Get current network risk (internal transaction analysis)
            internal_risk = network_findings.get("risk_score", 0.0)
            
            if len(provider_data) > 1:
                # CRITICAL FIX: Apply provider conflict resolution (most conservative)
                unified_assessment = resolve_provider_conflicts(provider_data)
                external_risk = unified_assessment.get("unified_risk_score", 0.0)
                conservative_provider = unified_assessment.get("consensus_provider", "unknown")
                
                # Apply intel fusion with unified assessment
                fused_risk = fuse_network_risk(internal_risk, unified_assessment.get("base_data", {}))
                
                # Add evidence of conflict resolution and fusion
                evidence = network_findings.setdefault("evidence", [])
                evidence.append(f"Applied provider conflict resolution: selected {conservative_provider} "
                              f"(risk: {fmt_num(external_risk, 3)}) as most conservative")
                evidence.append(f"Applied intel fusion with unified assessment (weighted internal:external = 70:30)")
                
                logger.info(f"Provider conflict resolution + fusion: internal={fmt_num(internal_risk, 3)}, "
                           f"unified_external={fmt_num(external_risk, 3)} -> fused={fmt_num(fused_risk, 3)}")
                
            else:
                # Single provider - apply standard fusion
                provider_name = list(provider_data.keys())[0]
                provider_payload = list(provider_data.values())[0]
                
                fused_risk = fuse_network_risk(internal_risk, provider_payload)
                
                # Add evidence of single provider fusion
                evidence = network_findings.setdefault("evidence", [])
                evidence.append(f"Applied intel fusion with {provider_name} (weighted internal:external = 70:30)")
                
                logger.info(f"Single provider fusion ({provider_name}): internal={fmt_num(internal_risk, 3)} -> fused={fmt_num(fused_risk, 3)}")
            
            # Update network domain risk score
            network_findings["risk_score"] = fused_risk
            
    except Exception as e:
        logger.warning(f"Intel fusion failed, using internal risk only: {e}")


def _clean_llm_analysis_sections(state: Dict[str, Any]) -> None:
    """
    Clean duplicate boilerplate from all LLM analysis sections in domain findings.
    """
    try:
        domain_findings = state.get("domain_findings", {})
        
        for domain_name, domain_data in domain_findings.items():
            if isinstance(domain_data, dict) and "llm_analysis" in domain_data:
                # Clean the analysis sections
                domain_data["llm_analysis"] = clean_llm_analysis_sections(domain_data["llm_analysis"])
                
        logger.debug("LLM analysis sections cleaned")
        
    except Exception as e:
        logger.warning(f"Failed to clean LLM analysis sections: {e}")


def _apply_confirmed_fraud_floors(state: Dict[str, Any]) -> None:
    """
    Apply confirmed fraud floor to logs and network domains.
    
    This ensures that confirmed fraud signals aren't accidentally suppressed
    by other risk calculations or fusion algorithms.
    """
    try:
        domain_findings = state.get("domain_findings", {})
        
        # Apply to logs and network domains (most likely to have fraud signals)
        for domain_name in ["logs", "network"]:
            domain_obj = domain_findings.get(domain_name)
            if isinstance(domain_obj, dict):
                apply_confirmed_fraud_floor(domain_obj)
                logger.debug(f"Applied confirmed fraud floor check to {domain_name} domain")
        
    except Exception as e:
        logger.warning(f"Failed to apply confirmed fraud floors: {e}")


def _ensure_duration_for_finalization(state: Dict[str, Any]) -> None:
    """
    Ensure total_duration_ms is set before finalization sanity checks.
    
    Uses the same logic as summary generator but called earlier to prevent
    assertion failures during risk finalization.
    """
    if state.get("total_duration_ms") is not None:
        return  # Already set, trust it
        
    # Try internal timer first (most authoritative)
    timer_elapsed = state.get("_timer_elapsed_ms")
    if timer_elapsed is not None:
        state["total_duration_ms"] = int(timer_elapsed)
        return
        
    # Fallback: compute from timestamps if available  
    start_time = state.get("start_time")
    end_time = state.get("end_time")
    if start_time and end_time:
        try:
            from datetime import datetime, timezone
            
            # Handle various ISO format variations
            start_str = start_time.replace('Z', '+00:00') if 'Z' in str(start_time) else str(start_time)
            end_str = end_time.replace('Z', '+00:00') if 'Z' in str(end_time) else str(end_time)
            
            dt_start = datetime.fromisoformat(start_str)
            dt_end = datetime.fromisoformat(end_str)
            
            # Ensure timezone awareness
            if dt_start.tzinfo is None:
                dt_start = dt_start.replace(tzinfo=timezone.utc)
            if dt_end.tzinfo is None:
                dt_end = dt_end.replace(tzinfo=timezone.utc)
            
            duration_ms = max(int((dt_end - dt_start).total_seconds() * 1000), 0)
            state["total_duration_ms"] = duration_ms
            return
            
        except Exception as e:
            logger.warning(f"Timestamp duration calculation failed: {e}")
    
    # Ultimate fallback: set to 0 (no crash)
    state["total_duration_ms"] = 0
    logger.debug("Set total_duration_ms to 0 as ultimate fallback")


def _assert_sanity_checks(state: Dict[str, Any], final_risk: float) -> None:
    """
    Perform sanity checks and assertions to catch regressions.
    
    Args:
        state: Investigation state
        final_risk: Computed final risk score
        
    Raises:
        AssertionError: If sanity checks fail
    """
    # Check duration is present and valid
    duration_ms = state.get("total_duration_ms")
    assert duration_ms is not None, "total_duration_ms missing"
    assert isinstance(duration_ms, (int, float)), f"total_duration_ms not numeric: {type(duration_ms)}"
    assert duration_ms >= 0, f"negative duration: {duration_ms}"
    
    # Check risk score is valid
    assert isinstance(final_risk, (int, float)), f"final_risk not numeric: {type(final_risk)}"
    assert 0.0 <= final_risk <= 1.0, f"final_risk out of range: {final_risk}"
    
    # Check domain risk scores are valid
    domain_findings = state.get("domain_findings", {})
    for domain_name, domain_data in domain_findings.items():
        if isinstance(domain_data, dict) and "risk_score" in domain_data:
            domain_risk = domain_data["risk_score"]
            assert isinstance(domain_risk, (int, float)), f"domain {domain_name} risk_score not numeric: {type(domain_risk)}"
            assert 0.0 <= domain_risk <= 1.0, f"domain {domain_name} risk_score out of range: {domain_risk}"
    
    # Check start/end times exist - DEFENSIVE: add end_time if missing
    assert state.get("start_time"), "start_time missing"
    
    # CRITICAL FIX: Add end_time defensively if missing (prevents report generation crashes)
    if not state.get("end_time"):
        logger.warning("end_time missing - computing defensively to prevent crash")
        
        start_time_str = state.get("start_time")
        duration_ms = state.get("total_duration_ms", 0)
        
        if start_time_str and duration_ms > 0:
            try:
                from datetime import datetime, timezone, timedelta
                
                # Parse start time (handle various ISO format variations)
                start_str = start_time_str.replace('Z', '+00:00') if 'Z' in start_time_str else start_time_str
                dt_start = datetime.fromisoformat(start_str)
                
                # Ensure timezone awareness (add UTC if missing)
                if dt_start.tzinfo is None:
                    dt_start = dt_start.replace(tzinfo=timezone.utc)
                
                # Compute end_time = start_time + duration
                dt_end = dt_start + timedelta(milliseconds=duration_ms)
                
                # Format as UTC ISO-8601
                computed_end_time = dt_end.isoformat()
                state["end_time"] = computed_end_time
                
                logger.info(f"‚úÖ DEFENSIVE: computed end_time = {computed_end_time} (start + {duration_ms}ms)")
                
            except Exception as e:
                logger.error(f"Failed to compute defensive end_time: {e}")
                # Last resort: use current time as end_time
                from datetime import datetime, timezone
                fallback_end_time = datetime.now(timezone.utc).isoformat()
                state["end_time"] = fallback_end_time
                logger.warning(f"‚ö†Ô∏è FALLBACK: Using current time as end_time = {fallback_end_time}")
        else:
            # No valid start_time or duration - use current time
            from datetime import datetime, timezone
            fallback_end_time = datetime.now(timezone.utc).isoformat()
            state["end_time"] = fallback_end_time
            logger.warning(f"‚ö†Ô∏è FALLBACK: No valid timing data, using current time as end_time = {fallback_end_time}")
    
    # Now we can safely assert end_time exists
    assert state.get("end_time"), "end_time still missing after defensive computation"
    
    # CRITICAL FIX: Add regression checks for key business logic
    _assert_business_logic_regressions(state)
    
    logger.debug("All sanity checks passed")


def _assert_business_logic_regressions(state: Dict[str, Any]) -> None:
    """
    Critical assertions to catch regressions in key business logic.
    
    User requirement: "Add asserts for key business logic (like: assert unique_ips >= 1 for IP investigations)"
    
    Args:
        state: Investigation state to validate
        
    Raises:
        AssertionError: If business logic regression is detected
    """
    try:
        # Check entity type and related assertions
        entity_type = state.get("entity_type", "").lower()
        entity_id = state.get("entity_id", "")
        
        # IP investigation business logic assertions
        if entity_type == "ip" or (entity_id and _looks_like_ip(entity_id)):
            domain_findings = state.get("domain_findings", {})
            network_findings = domain_findings.get("network", {})
            
            if network_findings:
                # CRITICAL: unique_ips must be >= 1 for IP investigations
                analysis = network_findings.get("analysis", {})  # FIXED: Look in analysis, not llm_analysis
                metrics = network_findings.get("metrics", {})
                
                unique_ips_analysis = analysis.get("unique_ips") if analysis else None
                unique_ips_metrics = metrics.get("unique_ip_count", 0) if metrics else 0
                
                # ROBUST FALLBACK: Fix unique_ips when missing or zero
                if unique_ips_analysis in (None, 0):
                    entity_ip = state.get("entity_id", "")
                    unique_ips_analysis = max(1, len(set([entity_ip])) if entity_ip else 1)
                    # Update the analysis for consistency
                    if analysis:
                        analysis["unique_ips"] = unique_ips_analysis
                
                assert unique_ips_analysis >= 1, f"IP investigation unique_ips in analysis must be >= 1, got {unique_ips_analysis}"
                assert unique_ips_metrics >= 1, f"IP investigation unique_ip_count in metrics must be >= 1, got {unique_ips_metrics}"
                
                # Consistency check between analysis and metrics
                assert unique_ips_analysis == unique_ips_metrics, (
                    f"unique_ips inconsistency: analysis={unique_ips_analysis}, metrics={unique_ips_metrics}"
                )
        
        # Evidence strength business logic assertions
        evidence_strength = state.get("evidence_strength", 0.0)
        assert isinstance(evidence_strength, (int, float)), f"evidence_strength must be numeric, got {type(evidence_strength)}"
        assert 0.0 <= evidence_strength <= 1.0, f"evidence_strength must be 0.0-1.0, got {evidence_strength}"
        
        # Tool results business logic assertions
        tools_used_raw = state.get("tools_used", [])
        tool_results = state.get("tool_results", {})
        
        # CRITICAL FIX: Extract tool names from tools_used (can be strings or dicts) to prevent "unhashable type: 'dict'" error
        tools_used = []
        for tool in tools_used_raw:
            if isinstance(tool, dict):
                # Extract tool name from dictionary
                tool_name = tool.get('tool_name') or tool.get('name') or str(tool)
            else:
                tool_name = str(tool)
            # Ensure it's a string before adding to list
            if isinstance(tool_name, str):
                tools_used.append(tool_name)
            else:
                tools_used.append(str(tool_name))
        
        # CRITICAL FIX: Check if tools_used have corresponding results
        # Some tools might be attempted but fail or return empty results
        if tools_used:
            # Check if at least some tools have results
            tools_with_results = [tool for tool in tools_used if tool in tool_results]
            if len(tools_with_results) == 0 and len(tool_results) == 0:
                # No results at all - this is a problem
                logger.warning(f"‚ö†Ô∏è Business logic warning: tools_used={len(tools_used)} but no tool_results found. Tools: {tools_used[:5]}")
                # Don't assert - this might be valid if all tools failed or were skipped
            elif len(tools_with_results) < len(tools_used):
                # Some tools don't have results - log warning but don't fail
                missing_tools = [tool for tool in tools_used if tool not in tool_results]
                logger.debug(f"üìä Some tools in tools_used don't have results: {missing_tools[:3]}")
        
        # Snowflake query business logic assertions
        snowflake_data = state.get("snowflake_data", {})
        if snowflake_data and snowflake_data.get("results"):
            results = snowflake_data["results"]
            assert isinstance(results, list), f"snowflake results must be list, got {type(results)}"
            
            # CRITICAL: Verify IS_FRAUD_TX is NOT present in results (data leakage prevention)
            if results:
                first_record = results[0]
                if isinstance(first_record, dict):
                    # This is a critical assertion - IS_FRAUD_TX must NOT be in investigation results
                    has_fraud_column = "IS_FRAUD_TX" in first_record
                    if has_fraud_column:
                        logger.error("üö® CRITICAL DATA LEAKAGE: IS_FRAUD_TX column found in Snowflake results - this should NOT happen!")
                        logger.error("   Investigation queries must exclude IS_FRAUD_TX to prevent data leakage")
                    else:
                        logger.debug("‚úÖ Data leakage check passed: IS_FRAUD_TX not found in results")
        
        # Domain analysis business logic assertions
        domain_findings = state.get("domain_findings", {})
        expected_domains = ["network", "device", "location", "logs", "authentication", "anomaly"]
        
        for domain in expected_domains:
            if domain in domain_findings:
                domain_data = domain_findings[domain]
                assert isinstance(domain_data, dict), f"domain {domain} data must be dict, got {type(domain_data)}"
                
                # Each domain should have evidence list
                evidence = domain_data.get("evidence", [])
                assert isinstance(evidence, list), f"domain {domain} evidence must be list, got {type(evidence)}"
        
        # Performance assertions (investigation should not take too long)
        duration_ms = state.get("total_duration_ms", 0)
        max_duration_ms = 300000  # 5 minutes max
        assert duration_ms <= max_duration_ms, f"investigation took too long: {duration_ms}ms > {max_duration_ms}ms"
        
        logger.debug("‚úÖ All business logic regression checks passed")
        
    except Exception as e:
        logger.error(f"‚ùå Business logic regression detected: {e}")
        raise


def _looks_like_ip(value: str) -> bool:
    """Check if a value looks like an IP address."""
    try:
        import ipaddress
        ipaddress.ip_address(value)
        return True
    except (ipaddress.AddressValueError, ValueError):
        return False


def _compute_evidence_strength(state: Dict[str, Any]) -> None:
    """
    Compute evidence strength as bounded blend of source diversity, corroboration, 
    volume/recency, and data quality.
    
    Evidence strength calculation (bounded 0.0-1.0):
    - Source diversity (0.0-0.4): Internal only=0.1, +1 external=0.25, +2 externals=0.4
    - Corroboration (0.0-0.3): Agreement between sources
    - Volume/recency (0.0-0.2): Data freshness and volume
    - Data quality (0.0-0.1): Completeness and consistency
    
    Args:
        state: Investigation state to update with evidence_strength
    """
    try:
        # Component scores
        source_diversity_score = 0.0
        corroboration_score = 0.0
        volume_recency_score = 0.0
        data_quality_score = 0.0
        
        # 1. SOURCE DIVERSITY (0.0-0.4)
        # CRITICAL FIX: Extract tool names from tools_used (can be strings or dicts) to prevent "unhashable type: 'dict'" error
        tools_used_raw = state.get("tools_used", [])
        tools_used = set()
        for tool in tools_used_raw:
            if isinstance(tool, dict):
                # Extract tool name from dictionary
                tool_name = tool.get('tool_name') or tool.get('name') or str(tool)
            else:
                tool_name = str(tool)
            # Ensure it's a string before adding to set
            if isinstance(tool_name, str):
                tools_used.add(tool_name)
            else:
                tools_used.add(str(tool_name))
        internal_sources = {"snowflake_query_tool"}
        external_sources = {"abuseipdb_ip_reputation", "virustotal_ip_analysis", "hybrid_analysis", "urlvoid"}
        
        has_internal = bool(tools_used & internal_sources)
        external_count = len(tools_used & external_sources)
        
        if has_internal and external_count == 0:
            source_diversity_score = 0.1  # Internal only - insufficient
        elif has_internal and external_count == 1:
            source_diversity_score = 0.25  # Internal + 1 external - marginal
        elif has_internal and external_count >= 2:
            source_diversity_score = 0.4  # Internal + 2+ externals - good
        elif external_count >= 2:
            source_diversity_score = 0.35  # Multiple externals, no internal - decent
        else:
            source_diversity_score = 0.1  # Insufficient diversity
        
        # 2. CORROBORATION (0.0-0.3)
        domain_findings = state.get("domain_findings", {})
        risk_scores = []
        for domain_data in domain_findings.values():
            if isinstance(domain_data, dict) and "risk_score" in domain_data:
                score = domain_data["risk_score"]
                if score is not None and 0 <= score <= 1:
                    risk_scores.append(score)
        
        if len(risk_scores) >= 2:
            # Calculate agreement between domain scores
            mean_score = safe_mean(risk_scores)
            max_deviation = max(abs(score - mean_score) for score in risk_scores) if risk_scores else 0
            if max_deviation <= 0.1:
                corroboration_score = 0.3  # High agreement
            elif max_deviation <= 0.3:
                corroboration_score = 0.2  # Moderate agreement  
            else:
                corroboration_score = 0.1  # Low agreement
        elif len(risk_scores) == 1:
            corroboration_score = 0.05  # Single score, no corroboration
        
        # 3. VOLUME/RECENCY (0.0-0.2)
        snowflake_data = state.get("snowflake_data", {})
        if isinstance(snowflake_data, dict) and "results" in snowflake_data:
            results = snowflake_data["results"]
            if isinstance(results, list) and len(results) > 0:
                volume_recency_score = min(0.2, len(results) * 0.05)  # More records = better
            else:
                volume_recency_score = 0.05  # Has Snowflake but no results
        
        # 4. DATA QUALITY (0.0-0.1)
        total_evidence_points = 0
        for domain_data in domain_findings.values():
            if isinstance(domain_data, dict):
                evidence_list = domain_data.get("evidence", [])
                total_evidence_points += len(evidence_list) if isinstance(evidence_list, list) else 0
        
        if total_evidence_points >= 5:
            data_quality_score = 0.1
        elif total_evidence_points >= 2:
            data_quality_score = 0.05
        
        # COMPUTE BASE EVIDENCE STRENGTH
        base_evidence_strength = min(1.0, source_diversity_score + corroboration_score + volume_recency_score + data_quality_score)
        
        # CRITICAL FIX: Add fraud pattern scoring as independent risk factor
        # Fraud patterns are PRIMARY indicators that override low evidence_strength
        fraud_pattern_score = 0.0
        snowflake_data = state.get("snowflake_data", {})
        
        if snowflake_data and isinstance(snowflake_data, dict) and "results" in snowflake_data:
            try:
                from app.service.agent.orchestration.risk.fraud_pattern_detectors import calculate_fraud_pattern_score
                
                results = snowflake_data["results"]
                if isinstance(results, list) and len(results) >= 3:
                    fraud_pattern_score, pattern_details = calculate_fraud_pattern_score(results)
                    
                    if fraud_pattern_score > 0.0:
                        logger.info(f"üîç Fraud pattern score: {fmt_num(fraud_pattern_score, 3)}")
                        logger.info(f"   Patterns detected: velocity={pattern_details['velocity_burst'] is not None}, "
                                   f"clustering={pattern_details['amount_clustering'] is not None}, "
                                   f"rotation={pattern_details['ip_rotation'] is not None}")
            except Exception as e:
                logger.debug(f"Fraud pattern scoring failed: {e}")
                fraud_pattern_score = 0.0
        
        # Override low evidence_strength when fraud patterns are strong
        # Strong patterns (‚â•0.6) boost evidence_strength to at least 0.6
        # Medium patterns (‚â•0.4) boost evidence_strength to at least 0.4
        if fraud_pattern_score >= 0.6 and base_evidence_strength < 0.6:
            evidence_strength = max(base_evidence_strength, 0.6)
            logger.info(f"‚úÖ Fraud pattern boost: evidence_strength elevated from {fmt_num(base_evidence_strength, 3)} to {fmt_num(evidence_strength, 3)} (pattern score: {fmt_num(fraud_pattern_score, 3)})")
        elif fraud_pattern_score >= 0.4 and base_evidence_strength < 0.4:
            evidence_strength = max(base_evidence_strength, 0.4)
            logger.info(f"‚úÖ Fraud pattern boost: evidence_strength elevated from {fmt_num(base_evidence_strength, 3)} to {fmt_num(evidence_strength, 3)} (pattern score: {fmt_num(fraud_pattern_score, 3)})")
        else:
            evidence_strength = base_evidence_strength
        
        # Store fraud pattern score in state for reference
        state["fraud_pattern_score"] = fraud_pattern_score
        
        # Update state
        state["evidence_strength"] = evidence_strength
        
        logger.info(
            f"Evidence strength computed: {fmt_num(evidence_strength, 3)} "
            f"(diversity: {fmt_num(source_diversity_score, 2)}, "
            f"corroboration: {fmt_num(corroboration_score, 2)}, "
            f"volume: {fmt_num(volume_recency_score, 2)}, "
            f"quality: {fmt_num(data_quality_score, 2)}) "
            f"[internal: {has_internal}, external: {external_count}, domains: {len(risk_scores)}]"
        )
        
    except Exception as e:
        # Fallback to minimal evidence strength on error
        state["evidence_strength"] = 0.1
        logger.warning(f"Evidence strength computation failed, using fallback: {e}")


def _deduplicate_cross_domain_prose(state: Dict[str, Any]) -> None:
    """
    Deduplicate prose across all domain findings using paragraph hashing.
    
    Analyzes LLM-generated text across all domains and removes duplicate
    paragraphs to prevent repetitive boilerplate across domain findings.
    
    Args:
        state: Investigation state containing domain_findings to deduplicate
    """
    try:
        import hashlib
        
        domain_findings = state.get("domain_findings", {})
        if not domain_findings:
            return
        
        # Collect all text paragraphs across domains with their locations
        paragraph_registry = {}  # hash -> (domain, section, paragraph_text, first_occurrence)
        duplicates_found = []
        
        # Phase 1: Catalog all paragraphs
        for domain_name, domain_data in domain_findings.items():
            if not isinstance(domain_data, dict):
                continue
                
            llm_analysis = domain_data.get("llm_analysis", {})
            if not isinstance(llm_analysis, dict):
                continue
            
            # Check key text sections
            for section_name in ["risk_factors", "reasoning", "recommendations"]:
                section_text = llm_analysis.get(section_name, "")
                if not isinstance(section_text, str) or len(section_text.strip()) < 20:
                    continue
                
                # Split into paragraphs (separated by double newlines or bullet points)
                paragraphs = []
                for para in section_text.split("\n\n"):
                    para = para.strip()
                    if len(para) >= 20:  # Only consider substantial paragraphs
                        paragraphs.append(para)
                
                # Hash each paragraph for duplicate detection
                for para_text in paragraphs:
                    # Normalize text for comparison (remove minor formatting differences)
                    normalized = para_text.lower().strip()
                    normalized = ' '.join(normalized.split())  # Normalize whitespace
                    para_hash = hashlib.md5(normalized.encode()).hexdigest()[:12]
                    
                    location = f"{domain_name}.{section_name}"
                    
                    if para_hash in paragraph_registry:
                        # Duplicate found!
                        first_location = paragraph_registry[para_hash][3]
                        duplicates_found.append({
                            "hash": para_hash,
                            "text": para_text[:80] + "..." if len(para_text) > 80 else para_text,
                            "first_location": first_location,
                            "duplicate_location": location
                        })
                    else:
                        # First occurrence
                        paragraph_registry[para_hash] = (domain_name, section_name, para_text, location)
        
        # Phase 2: Remove duplicates (keep first occurrence only)
        removed_count = 0
        for duplicate_info in duplicates_found:
            para_hash = duplicate_info["hash"]
            duplicate_location = duplicate_info["duplicate_location"]
            
            # Parse duplicate location
            domain_name, section_name = duplicate_location.split(".", 1)
            original_para_text = paragraph_registry[para_hash][2]
            
            # Get the section text and remove this paragraph
            domain_data = domain_findings[domain_name]
            llm_analysis = domain_data["llm_analysis"]
            section_text = llm_analysis[section_name]
            
            # Remove the duplicate paragraph from the section
            if original_para_text in section_text:
                # Replace the paragraph with empty string and clean up
                new_section_text = section_text.replace(original_para_text, "")
                # Clean up extra blank lines and spacing
                new_section_text = "\n\n".join([p.strip() for p in new_section_text.split("\n\n") if p.strip()])
                llm_analysis[section_name] = new_section_text
                removed_count += 1
        
        if removed_count > 0:
            logger.info(f"Cross-domain deduplication: removed {removed_count} duplicate paragraphs")
            logger.debug(f"Duplicates removed from: {[d['duplicate_location'] for d in duplicates_found]}")
        else:
            logger.debug("Cross-domain deduplication: no duplicate paragraphs found")
            
    except Exception as e:
        logger.warning(f"Cross-domain prose deduplication failed: {e}")


def _apply_decisive_evidence_policy(state: Dict[str, Any]) -> None:
    """
    Apply decisive evidence policy for confidence floors.
    
    When evidence_strength >= 0.7 AND we have confirmed fraud indicators,
    ensure confidence >= 0.3 to improve QA scores for high-quality investigations.
    
    Args:
        state: Investigation state to apply confidence floors to
    """
    try:
        evidence_strength = state.get("evidence_strength", 0.0)
        
        # Only apply if we have strong evidence
        if evidence_strength < 0.7:
            logger.debug(f"Decisive evidence policy: evidence strength {fmt_num(evidence_strength, 3)} < 0.7, no confidence floor applied")
            return
        
        # Check for confirmed fraud across domains
        has_confirmed_fraud = False
        domain_findings = state.get("domain_findings", {})
        
        for domain_name, domain_data in domain_findings.items():
            if not isinstance(domain_data, dict):
                continue
                
            # Check LLM analysis for confirmed fraud
            llm_analysis = domain_data.get("llm_analysis", {})
            if isinstance(llm_analysis, dict):
                risk_factors = llm_analysis.get("risk_factors", "")
                reasoning = llm_analysis.get("reasoning", "")
                combined_text = (str(risk_factors) + " " + str(reasoning)).lower()
                
                if "confirmed fraud" in combined_text:
                    has_confirmed_fraud = True
                    break
            
            # CRITICAL: No fraud indicators can be used during investigation to prevent data leakage
            # All fraud indicator columns (IS_FRAUD_TX, COUNT_DISPUTES, COUNT_FRAUD_ALERTS, etc.) are excluded
            # Confirmed fraud detection must be based on LLM analysis text only, not structured fraud flags
        
        if not has_confirmed_fraud:
            logger.debug("Decisive evidence policy: no confirmed fraud detected, no confidence floor applied")
            return
        
        # Apply confidence floor to domain findings
        confidence_floor = 0.3
        domains_updated = []
        
        for domain_name, domain_data in domain_findings.items():
            if isinstance(domain_data, dict):
                current_confidence = domain_data.get("confidence", 0.0)
                
                if isinstance(current_confidence, (int, float)) and current_confidence < confidence_floor:
                    domain_data["confidence"] = confidence_floor
                    domains_updated.append(domain_name)
                    logger.info(f"Applied confidence floor to {domain_name}: {fmt_num(current_confidence, 3)} -> {fmt_num(confidence_floor, 3)}")
        
        # Also apply to overall state confidence if it exists
        state_confidence = state.get("confidence", 0.0)
        if isinstance(state_confidence, (int, float)) and state_confidence < confidence_floor:
            state["confidence"] = confidence_floor
            logger.info(f"Applied confidence floor to investigation: {fmt_num(state_confidence, 3)} -> {fmt_num(confidence_floor, 3)}")
        
        if domains_updated or state_confidence < confidence_floor:
            logger.info(f"Decisive evidence policy applied: evidence_strength={fmt_num(evidence_strength, 3)}, confirmed_fraud=True, confidence_floor={fmt_num(confidence_floor, 3)}")
        else:
            logger.debug(f"Decisive evidence policy: conditions met but no confidence adjustments needed")
            
    except Exception as e:
        logger.warning(f"Failed to apply decisive evidence policy: {e}")


def _apply_evidence_gate(state: Dict[str, Any]) -> None:
    """
    Apply evidence gate to prevent Snowflake-only investigations from finalizing.
    
    CRITICAL FIX: Ensures investigation has sufficient external validation before
    finalizing. Prevents low-quality investigations based solely on Snowflake data.
    
    Args:
        state: Investigation state to validate
    """
    try:
        from .policy import has_minimum_evidence
        
        if not has_minimum_evidence(state):
            # Investigation lacks sufficient external evidence
            logger.error("üö´ EVIDENCE GATE VIOLATION: Investigation lacks minimum external evidence")
            logger.error("   This investigation relies too heavily on Snowflake data without external validation")
            
            # Mark investigation as incomplete/low-quality
            state["evidence_gate_passed"] = False
            state["investigation_quality"] = "insufficient_evidence"
            
            # Add warning to any existing risk indicators
            risk_indicators = state.setdefault("risk_indicators", [])
            risk_indicators.append("Investigation quality: Insufficient external validation")
            
            logger.warning("‚ö†Ô∏è Investigation marked as low-quality due to insufficient evidence")
        else:
            state["evidence_gate_passed"] = True
            state["investigation_quality"] = "sufficient_evidence"
            logger.debug("‚úÖ Evidence gate passed - investigation has sufficient external validation")
            
    except Exception as e:
        logger.warning(f"Evidence gate check failed, allowing investigation to proceed: {e}")
        state["evidence_gate_passed"] = None  # Unknown state


def _generate_action_plan(state: Dict[str, Any], final_risk: float) -> None:
    """
    Generate severity-appropriate action plan based on final risk assessment.
    
    CRITICAL FIX: Ensures recommendations match risk severity level to prevent
    generic recommendations that don't align with actual risk assessment.
    
    Args:
        state: Investigation state to update
        final_risk: Final computed risk score
    """
    try:
        from .policy import action_plan
        
        # Generate actions appropriate for the risk level
        actions = action_plan(final_risk, state)
        
        # Store action plan in state
        state["action_plan"] = actions
        state["recommended_actions"] = actions  # Legacy compatibility
        
        # Add summary for logging
        if actions:
            logger.info(f"üéØ Action plan generated: {len(actions)} recommendations for risk {fmt_num(final_risk, 3)}")
            logger.debug(f"   Actions: {actions[:3]}...")  # Show first 3 actions
        else:
            logger.warning(f"‚ö†Ô∏è No actions generated for risk {fmt_num(final_risk, 3)}")
        
    except Exception as e:
        logger.warning(f"Action plan generation failed, using fallback: {e}")
        # Fallback to basic recommendations
        if final_risk >= 0.7:
            state["action_plan"] = ["Immediate review required", "Escalate to fraud team"]
        elif final_risk >= 0.4:
            state["action_plan"] = ["Enhanced monitoring", "Consider additional verification"]
        else:
            state["action_plan"] = ["Routine monitoring", "Standard processing"]


def _apply_anti_flap_guard(entity_id: str, final_risk: float, state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Apply anti-flap guard to prevent wild risk score swings without new evidence.
    
    CRITICAL PATCH E: Integrates flap protection into risk finalization pipeline.
    
    Args:
        entity_id: Entity being investigated
        final_risk: Computed final risk score  
        state: Investigation state
        
    Returns:
        Dict with flap analysis and adjusted risk score
    """
    try:
        from .policy import check_anti_flap_guard
        
        # Apply the anti-flap guard
        flap_result = check_anti_flap_guard(entity_id, final_risk, state, threshold=0.3)
        
        # Log the result
        if flap_result.get("flap_detected"):
            logger.warning(f"üö® ANTI-FLAP GUARD ACTIVATED: {flap_result['provenance']}")
            # Add to investigation quality markers
            state.setdefault("investigation_warnings", []).append("Risk flapping detected and limited")
        else:
            logger.debug(f"‚úÖ Anti-flap guard: {flap_result['provenance']}")
        
        # Store flap analysis in state for audit trail
        state["anti_flap_analysis"] = {
            "flap_detected": flap_result.get("flap_detected", False),
            "original_risk": final_risk,
            "adjusted_risk": flap_result.get("adjusted_risk", final_risk),
            "evidence_hash": flap_result.get("evidence_hash"),
            "risk_delta": flap_result.get("risk_delta", 0.0),
            "evidence_changed": flap_result.get("evidence_changed", True),
            "provenance": flap_result.get("provenance", "No provenance available")
        }
        
        return flap_result
        
    except Exception as e:
        logger.error(f"Anti-flap guard failed for {entity_id}: {e}")
        # Safe fallback - allow the original risk
        return {
            "flap_detected": False,
            "adjusted_risk": final_risk,
            "error": str(e),
            "provenance": f"Anti-flap guard error - using original risk {fmt_num(final_risk, 3)}"
        }