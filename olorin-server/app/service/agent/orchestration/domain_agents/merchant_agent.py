"""
Merchant Domain Analysis Agent

Analyzes merchant patterns, risk levels, velocity, and merchant-related fraud indicators.
Uses Snowflake as the primary tool for finding merchant anomalies.
"""

import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from app.service.agent.orchestration.state_schema import (
    InvestigationState,
    add_domain_findings,
)
from app.service.agent.tools.snowflake_tool.schema_constants import (  # MERCHANT_RISK_LEVEL not in schema_constants - handled dynamically
    COUNT_MERCHANT_DECISIONS,
    DAYS_FROM_FIRST_MERCHANT_ACCEPTANCE_TO_TX,
    IS_FRAUD_TX,
    MERCHANT_DECISIONS,
    MERCHANT_LAST_DECISION,
    MERCHANT_LAST_DECISION_DATETIME,
    MERCHANT_NAME,
    MERCHANT_SEGMENT_ID,
    MODEL_SCORE,
    PAID_AMOUNT_VALUE_IN_CURRENCY,
    TX_DATETIME,
)
from app.service.logging import get_bridge_logger

from .base import (
    DomainAgentBase,
    complete_chain_of_thought,
    log_agent_handover_complete,
)

logger = get_bridge_logger(__name__)


async def merchant_agent_node(
    state: InvestigationState, config: Optional[Dict] = None
) -> Dict[str, Any]:
    """
    Merchant domain analysis agent.
    Analyzes merchant patterns, risk levels, velocity, and merchant-related fraud indicators.
    Uses Snowflake as the primary tool for finding merchant anomalies.
    """
    try:
        start_time = time.time()
        logger.info("[Step 5.2.7] ðŸª Merchant agent analyzing investigation")

        # Get relevant data from state
        snowflake_data = state.get("snowflake_data", {})
        tool_results = state.get("tool_results", {})
        entity_id = state["entity_id"]
        entity_type = state["entity_type"]
        investigation_id = state.get("investigation_id", "unknown")

        # Initialize logging and chain of thought
        DomainAgentBase.log_agent_start("merchant", entity_type, entity_id, False)
        DomainAgentBase.log_context_analysis(snowflake_data, tool_results, "merchant")

        process_id = DomainAgentBase.start_chain_of_thought(
            investigation_id=investigation_id,
            agent_name="merchant_agent",
            domain="merchant",
            entity_type=entity_type,
            entity_id=entity_id,
            task_description="Merchant patterns are critical fraud indicators revealing merchant risk associations, "
            "velocity patterns, and merchant decision anomalies. Will analyze: (1) Merchant risk level "
            "distribution and high-risk merchant associations, (2) Merchant velocity patterns (rapid "
            "merchant switching), (3) Merchant category clustering and suspicious combinations, "
            "(4) Merchant decision patterns and anomalies, (5) Merchant-fraud correlation patterns",
        )

        # Initialize merchant findings
        merchant_findings = DomainAgentBase.initialize_findings("merchant")

        # Process Snowflake data for merchant patterns
        results = DomainAgentBase.process_snowflake_results(snowflake_data, "merchant")

        logger.info(f"ðŸ“Š Merchant agent processing Snowflake data:")
        logger.info(f"   snowflake_data type: {type(snowflake_data).__name__}")
        if isinstance(snowflake_data, dict):
            logger.info(f"   snowflake_data keys: {list(snowflake_data.keys())}")
            if "results" in snowflake_data:
                logger.info(
                    f"   snowflake_data['results'] count: {len(snowflake_data.get('results', []))}"
                )

        if results:
            # Log actual record structure for debugging
            if results and isinstance(results[0], dict):
                logger.info(f"ðŸ“Š Sample record structure (first record):")
                logger.info(f"   Record keys: {list(results[0].keys())[:20]}")
                merchant_fields = [
                    k
                    for k in results[0].keys()
                    if "MERCHANT" in k.upper() or "merchant" in k.lower()
                ]
                logger.info(f"   Merchant-related fields: {merchant_fields}")

            # Analyze merchant risk distribution
            _analyze_merchant_risk_distribution(results, merchant_findings)

            # Analyze merchant velocity patterns
            _analyze_merchant_velocity(results, merchant_findings)

            # Analyze merchant category patterns
            _analyze_merchant_category_patterns(results, merchant_findings)

            # Analyze merchant decision patterns
            _analyze_merchant_decisions(results, merchant_findings)

            # Analyze merchant-fraud correlation
            _analyze_merchant_fraud_correlation(results, merchant_findings)

            # Ensure we always have some evidence
            if len(merchant_findings.get("evidence", [])) == 0:
                logger.warning(
                    "âš ï¸ No evidence collected from merchant analysis, adding basic transaction count"
                )
                merchant_findings["evidence"].append(
                    f"Merchant data analyzed: {len(results)} records processed"
                )
                merchant_findings["evidence"].append(
                    "Merchant analysis completed on transaction dataset"
                )

            logger.info(
                f"ðŸ“Š Evidence collected from Snowflake data: {len(merchant_findings.get('evidence', []))} items"
            )
        else:
            logger.warning(
                f"âš ï¸ Merchant agent: No results extracted from snowflake_data"
            )
            if isinstance(snowflake_data, str):
                merchant_findings["risk_indicators"].append(
                    "Snowflake data in non-structured format"
                )

        # Add evidence summary
        merchant_findings["evidence_summary"] = {
            "total_evidence_points": len(merchant_findings["evidence"]),
            "risk_indicators_found": len(merchant_findings["risk_indicators"]),
            "metrics_collected": len(merchant_findings["metrics"]),
        }

        # CRITICAL: Analyze evidence with LLM to generate risk scores (with ALL tool results)
        from .base import analyze_evidence_with_llm

        merchant_findings = await analyze_evidence_with_llm(
            domain="merchant",
            findings=merchant_findings,
            snowflake_data=snowflake_data,
            tool_results=tool_results,
            entity_type=entity_type,
            entity_id=entity_id,
        )

        # Finalize findings
        analysis_duration = time.time() - start_time
        DomainAgentBase.finalize_findings(
            merchant_findings,
            snowflake_data,
            tool_results,
            analysis_duration,
            "merchant",
        )

        # Complete logging
        log_agent_handover_complete("merchant", merchant_findings)
        complete_chain_of_thought(process_id, merchant_findings, "merchant")

        logger.info(
            f"[Step 5.2.7] âœ… Merchant analysis complete - Evidence collected: {len(merchant_findings['evidence'])} points"
        )

        # Run merchant validation automatically
        try:
            from app.service.logging.investigation_folder_manager import (
                InvestigationFolderManager,
            )

            from .merchant_validation import get_validation_service

            validation_service = await get_validation_service()

            # Get investigation folder path
            investigation_folder = None
            try:
                folder_manager = InvestigationFolderManager()
                investigation_folder = folder_manager.get_investigation_folder(
                    investigation_id
                )
            except Exception as e:
                logger.debug(f"Could not get investigation folder: {e}")

            # Run validation
            validation_results = await validation_service.run_validation(
                investigation_id=investigation_id,
                entity_type=entity_type,
                entity_id=entity_id,
                merchant_findings=merchant_findings,
                investigation_folder=investigation_folder,
            )

            # Add validation results to merchant findings
            merchant_findings["validation"] = validation_results
            logger.info(f"[Step 5.2.7] âœ… Merchant validation completed")

        except Exception as e:
            logger.warning(f"[Step 5.2.7] âš ï¸ Merchant validation failed: {e}")
            # Continue without validation - don't fail the agent

        # Update state with findings
        return add_domain_findings(state, "merchant", merchant_findings)

    except Exception as e:
        logger.error(f"âŒ Merchant agent failed: {str(e)}", exc_info=True)

        # Record failure with circuit breaker
        from app.service.agent.orchestration.circuit_breaker import record_node_failure

        record_node_failure(state, "merchant_agent", e)

        # Return state as-is to allow investigation to continue
        return state


def _analyze_merchant_risk_distribution(
    results: List[Dict], findings: Dict[str, Any]
) -> None:
    """Analyze distribution of merchant risk levels."""
    # Support both uppercase (Snowflake) and lowercase (PostgreSQL) column names
    merchant_risk_levels = {}
    merchant_names = {}
    total_transactions = len(results)
    fraud_transactions = 0

    for r in results:
        # Get merchant name
        merchant_name = r.get("MERCHANT_NAME") or r.get("merchant_name") or "UNKNOWN"

        # Get merchant risk level
        risk_level = (
            r.get("MERCHANT_RISK_LEVEL") or r.get("merchant_risk_level") or "UNKNOWN"
        )

        # Count by risk level
        merchant_risk_levels[risk_level] = merchant_risk_levels.get(risk_level, 0) + 1

        # Track merchant names
        if merchant_name not in merchant_names:
            merchant_names[merchant_name] = {
                "count": 0,
                "risk_level": risk_level,
                "fraud_count": 0,
                "total_amount": 0.0,
            }

        merchant_names[merchant_name]["count"] += 1

        # CRITICAL: IS_FRAUD_TX removed - must not use ground truth labels during investigation
        # Use decision-based indicators instead (BLOCK/REJECT decisions indicate high risk)
        decision = r.get("NSURE_LAST_DECISION") or r.get("nsure_last_decision") or ""
        if decision in ("BLOCK", "REJECT", "DECLINE"):
            fraud_transactions += 1
            merchant_names[merchant_name]["fraud_count"] += 1

        # Track amounts
        amount = (
            r.get("PAID_AMOUNT_VALUE_IN_CURRENCY")
            or r.get("paid_amount_value_in_currency")
            or 0.0
        )
        try:
            merchant_names[merchant_name]["total_amount"] += (
                float(amount) if amount else 0.0
            )
        except (ValueError, TypeError):
            pass

    # Store metrics
    findings["metrics"]["unique_merchants"] = len(merchant_names)
    findings["metrics"]["total_transactions"] = total_transactions
    findings["metrics"]["fraud_transactions"] = fraud_transactions
    findings["metrics"]["merchant_risk_distribution"] = merchant_risk_levels

    # Analyze risk distribution
    high_risk_count = merchant_risk_levels.get("high", 0) + merchant_risk_levels.get(
        "HIGH", 0
    )
    medium_risk_count = merchant_risk_levels.get(
        "medium", 0
    ) + merchant_risk_levels.get("MEDIUM", 0)
    low_risk_count = merchant_risk_levels.get("low", 0) + merchant_risk_levels.get(
        "LOW", 0
    )

    findings["evidence"].append(
        f"Unique merchants: {len(merchant_names)} across {total_transactions} transactions"
    )
    findings["evidence"].append(
        f"Merchant risk distribution: High={high_risk_count}, Medium={medium_risk_count}, Low={low_risk_count}"
    )

    # Risk indicators
    if high_risk_count > 0:
        high_risk_pct = (
            (high_risk_count / total_transactions) * 100
            if total_transactions > 0
            else 0
        )
        findings["evidence"].append(
            f"High-risk merchant transactions: {high_risk_count} ({high_risk_pct:.1f}%)"
        )

        if high_risk_pct > 50:
            findings["risk_indicators"].append(
                f"High concentration of high-risk merchant transactions ({high_risk_pct:.1f}%)"
            )
            findings["evidence"].append(
                f"SUSPICIOUS: >50% transactions with high-risk merchants"
            )

    # Identify merchants with fraud
    fraud_merchants = {
        name: data for name, data in merchant_names.items() if data["fraud_count"] > 0
    }
    if fraud_merchants:
        findings["evidence"].append(f"Merchants with fraud: {len(fraud_merchants)}")
        findings["risk_indicators"].append(
            f"{len(fraud_merchants)} merchants associated with fraud transactions"
        )

        # List top fraud merchants
        top_fraud_merchants = sorted(
            fraud_merchants.items(), key=lambda x: x[1]["fraud_count"], reverse=True
        )[:5]
        for name, data in top_fraud_merchants:
            findings["evidence"].append(
                f"  - {name}: {data['fraud_count']} fraud transactions, risk={data['risk_level']}"
            )

    findings["analysis"]["merchant_risk_distribution"] = merchant_risk_levels
    findings["analysis"]["merchant_names"] = {
        name: {"count": data["count"], "risk_level": data["risk_level"]}
        for name, data in list(merchant_names.items())[:10]
    }


def _analyze_merchant_velocity(results: List[Dict], findings: Dict[str, Any]) -> None:
    """Detect merchant velocity patterns (rapid merchant switching)."""
    # Group transactions by merchant and timestamp
    merchant_timestamps = {}

    for r in results:
        merchant_name = r.get("MERCHANT_NAME") or r.get("merchant_name") or "UNKNOWN"
        tx_datetime_str = r.get("TX_DATETIME") or r.get("tx_datetime")

        if tx_datetime_str:
            try:
                # Parse datetime (handle various formats)
                if isinstance(tx_datetime_str, str):
                    # Try ISO format first
                    if "T" in tx_datetime_str or " " in tx_datetime_str:
                        tx_datetime = datetime.fromisoformat(
                            tx_datetime_str.replace("Z", "+00:00")
                        )
                    else:
                        tx_datetime = datetime.fromisoformat(tx_datetime_str)
                else:
                    tx_datetime = tx_datetime_str

                if merchant_name not in merchant_timestamps:
                    merchant_timestamps[merchant_name] = []
                merchant_timestamps[merchant_name].append(tx_datetime)
            except Exception as e:
                logger.debug(f"Could not parse datetime {tx_datetime_str}: {e}")
                continue

    if not merchant_timestamps:
        findings["evidence"].append(
            "Merchant velocity analysis: No timestamp data available"
        )
        return

    # Sort timestamps for each merchant
    for merchant_name in merchant_timestamps:
        merchant_timestamps[merchant_name].sort()

    # Calculate velocity metrics
    unique_merchants = len(merchant_timestamps)
    total_transactions = len(results)

    # Detect rapid merchant switching (multiple merchants in short time)
    if len(merchant_timestamps) > 1:
        # Get all timestamps sorted
        all_timestamps = []
        for timestamps in merchant_timestamps.values():
            all_timestamps.extend(
                [
                    (ts, merchant)
                    for merchant, timestamps_list in merchant_timestamps.items()
                    for ts in timestamps_list
                    if ts in timestamps_list
                ]
            )

        all_timestamps.sort(key=lambda x: x[0])

        # Detect rapid switching (multiple merchants within 1 hour)
        rapid_switches = 0
        merchants_in_hour = set()
        last_timestamp = None

        for timestamp, merchant in all_timestamps:
            if last_timestamp:
                time_diff = (timestamp - last_timestamp).total_seconds() / 3600  # hours
                if time_diff <= 1.0:  # Within 1 hour
                    merchants_in_hour.add(merchant)
                    if len(merchants_in_hour) >= 3:
                        rapid_switches += 1
                else:
                    merchants_in_hour = {merchant}
            else:
                merchants_in_hour = {merchant}

            last_timestamp = timestamp

        findings["metrics"]["merchant_velocity_rapid_switches"] = rapid_switches
        findings["metrics"]["unique_merchants"] = unique_merchants
        findings["metrics"]["merchants_per_transaction"] = (
            unique_merchants / total_transactions if total_transactions > 0 else 0
        )

        findings["evidence"].append(
            f"Merchant velocity: {unique_merchants} unique merchants across {total_transactions} transactions"
        )

        if rapid_switches > 0:
            findings["risk_indicators"].append(
                f"Rapid merchant switching detected: {rapid_switches} instances of 3+ merchants within 1 hour"
            )
            findings["evidence"].append(
                f"SUSPICIOUS: Rapid merchant switching pattern detected"
            )

        # First-time merchant usage
        if len(merchant_timestamps) > 1:
            first_merchant = min(merchant_timestamps.items(), key=lambda x: min(x[1]))[
                0
            ]
            findings["evidence"].append(f"First merchant used: {first_merchant}")

            # Check if many merchants are used only once
            single_use_merchants = sum(
                1 for timestamps in merchant_timestamps.values() if len(timestamps) == 1
            )
            if single_use_merchants > unique_merchants * 0.5:
                findings["risk_indicators"].append(
                    f"High single-use merchant rate: {single_use_merchants}/{unique_merchants} merchants used only once"
                )
                findings["evidence"].append(
                    f"SUSPICIOUS: >50% merchants used only once (potential card testing)"
                )
    else:
        findings["evidence"].append(
            f"Merchant velocity: Single merchant ({list(merchant_timestamps.keys())[0] if merchant_timestamps else 'UNKNOWN'})"
        )

    findings["analysis"]["merchant_velocity"] = {
        "unique_merchants": unique_merchants,
        "rapid_switches": findings["metrics"].get(
            "merchant_velocity_rapid_switches", 0
        ),
    }


def _analyze_merchant_category_patterns(
    results: List[Dict], findings: Dict[str, Any]
) -> None:
    """Analyze merchant category clustering and patterns."""
    # Note: MERCHANT_CATEGORY may not be in schema, but we'll check for it
    merchant_categories = {}
    category_merchants = {}

    for r in results:
        merchant_name = r.get("MERCHANT_NAME") or r.get("merchant_name") or "UNKNOWN"
        # Try different possible category field names
        category = (
            r.get("MERCHANT_CATEGORY")
            or r.get("merchant_category")
            or r.get("MERCHANT_CATEGORY_CODE")
            or r.get("merchant_category_code")
            or "UNKNOWN"
        )

        merchant_categories[merchant_name] = category

        if category not in category_merchants:
            category_merchants[category] = {
                "merchants": set(),
                "transactions": 0,
                "fraud_count": 0,
            }

        category_merchants[category]["merchants"].add(merchant_name)
        category_merchants[category]["transactions"] += 1

        # CRITICAL: IS_FRAUD_TX removed - must not use ground truth labels during investigation
        # Use decision-based indicators instead
        decision = r.get("NSURE_LAST_DECISION") or r.get("nsure_last_decision") or ""
        if decision in ("BLOCK", "REJECT", "DECLINE"):
            category_merchants[category]["fraud_count"] += 1

    if len(category_merchants) > 1:
        findings["metrics"]["unique_categories"] = len(category_merchants)
        findings["evidence"].append(
            f"Merchant categories: {len(category_merchants)} unique categories"
        )

        # Identify high-risk categories
        for category, data in category_merchants.items():
            if data["transactions"] > 0:
                fraud_rate = data["fraud_count"] / data["transactions"]
                if fraud_rate > 0.3:  # >30% fraud rate
                    findings["risk_indicators"].append(
                        f"High fraud rate category: {category} ({fraud_rate*100:.1f}% fraud)"
                    )
                    findings["evidence"].append(
                        f"SUSPICIOUS: Category '{category}' has {fraud_rate*100:.1f}% fraud rate"
                    )

        # Detect rapid category switching
        categories_used = list(set(merchant_categories.values()))
        if len(categories_used) > 3:
            findings["risk_indicators"].append(
                f"High category diversity: {len(categories_used)} different categories"
            )
            findings["evidence"].append(
                f"SUSPICIOUS: Transactions span {len(categories_used)} different categories"
            )

        findings["analysis"]["merchant_categories"] = {
            category: {
                "merchant_count": len(data["merchants"]),
                "transaction_count": data["transactions"],
                "fraud_rate": (
                    data["fraud_count"] / data["transactions"]
                    if data["transactions"] > 0
                    else 0
                ),
            }
            for category, data in category_merchants.items()
        }
    else:
        findings["evidence"].append(
            "Merchant category analysis: Single category or category data not available"
        )


def _analyze_merchant_decisions(results: List[Dict], findings: Dict[str, Any]) -> None:
    """Analyze merchant decision patterns and anomalies."""
    merchant_decisions = {}
    decision_counts = {}

    for r in results:
        merchant_name = r.get("MERCHANT_NAME") or r.get("merchant_name") or "UNKNOWN"

        # Get merchant decision
        decision = (
            r.get("MERCHANT_LAST_DECISION")
            or r.get("merchant_last_decision")
            or r.get("NSURE_LAST_DECISION")
            or r.get("nsure_last_decision")
            or "UNKNOWN"
        )

        if merchant_name not in merchant_decisions:
            merchant_decisions[merchant_name] = {
                "decisions": [],
                "decision_counts": {},
                "last_decision": decision,
            }

        merchant_decisions[merchant_name]["decisions"].append(decision)
        merchant_decisions[merchant_name]["decision_counts"][decision] = (
            merchant_decisions[merchant_name]["decision_counts"].get(decision, 0) + 1
        )

        # Track overall decision counts
        decision_counts[decision] = decision_counts.get(decision, 0) + 1

    findings["metrics"]["merchant_decision_distribution"] = decision_counts
    findings["evidence"].append(f"Merchant decisions: {dict(decision_counts)}")

    # Detect decision anomalies
    for merchant_name, data in merchant_decisions.items():
        decisions = data["decisions"]
        if len(set(decisions)) > 1:  # Multiple different decisions
            findings["evidence"].append(
                f"Merchant {merchant_name}: Multiple decision types {set(decisions)}"
            )

        # Check for unusual decision patterns
        accept_count = sum(
            1
            for d in decisions
            if "accept" in str(d).lower() or "approve" in str(d).lower()
        )
        reject_count = sum(
            1
            for d in decisions
            if "reject" in str(d).lower()
            or "decline" in str(d).lower()
            or "block" in str(d).lower()
        )

        if reject_count > accept_count * 2:  # More than 2x rejections vs acceptances
            findings["risk_indicators"].append(
                f"Merchant {merchant_name}: High rejection rate ({reject_count} rejections vs {accept_count} acceptances)"
            )
            findings["evidence"].append(
                f"SUSPICIOUS: Merchant {merchant_name} has high rejection rate"
            )

    findings["analysis"]["merchant_decisions"] = {
        merchant: {
            "decision_distribution": data["decision_counts"],
            "decision_diversity": len(set(data["decisions"])),
        }
        for merchant, data in list(merchant_decisions.items())[:10]
    }


def _analyze_merchant_fraud_correlation(
    results: List[Dict], findings: Dict[str, Any]
) -> None:
    """Analyze correlation between merchant patterns and fraud outcomes."""
    merchant_fraud_data = {}

    for r in results:
        merchant_name = r.get("MERCHANT_NAME") or r.get("merchant_name") or "UNKNOWN"
        # CRITICAL: IS_FRAUD_TX removed - must not use ground truth labels during investigation
        # Use decision-based indicators instead
        decision = r.get("NSURE_LAST_DECISION") or r.get("nsure_last_decision") or ""
        is_high_risk = decision in ("BLOCK", "REJECT", "DECLINE")
        model_score = r.get("MODEL_SCORE") or r.get("model_score")
        amount = (
            r.get("PAID_AMOUNT_VALUE_IN_CURRENCY")
            or r.get("paid_amount_value_in_currency")
            or 0.0
        )

        if merchant_name not in merchant_fraud_data:
            merchant_fraud_data[merchant_name] = {
                "total": 0,
                "fraud": 0,
                "model_scores": [],
                "amounts": [],
            }

        merchant_fraud_data[merchant_name]["total"] += 1

        # Use decision-based high risk indicator instead of IS_FRAUD_TX
        if is_high_risk:
            merchant_fraud_data[merchant_name]["fraud"] += 1

        if model_score is not None:
            try:
                merchant_fraud_data[merchant_name]["model_scores"].append(
                    float(model_score)
                )
            except (ValueError, TypeError):
                pass

        try:
            merchant_fraud_data[merchant_name]["amounts"].append(
                float(amount) if amount else 0.0
            )
        except (ValueError, TypeError):
            pass

    # Calculate fraud rates per merchant
    high_fraud_merchants = []
    for merchant_name, data in merchant_fraud_data.items():
        if data["total"] > 0:
            fraud_rate = data["fraud"] / data["total"]
            avg_model_score = (
                sum(data["model_scores"]) / len(data["model_scores"])
                if data["model_scores"]
                else 0.0
            )

            if fraud_rate > 0.2:  # >20% fraud rate
                high_fraud_merchants.append(
                    {
                        "merchant": merchant_name,
                        "fraud_rate": fraud_rate,
                        "fraud_count": data["fraud"],
                        "total": data["total"],
                        "avg_model_score": avg_model_score,
                    }
                )

    if high_fraud_merchants:
        findings["risk_indicators"].append(
            f"{len(high_fraud_merchants)} merchants with >20% fraud rate"
        )
        findings["evidence"].append(
            f"High fraud rate merchants: {len(high_fraud_merchants)}"
        )

        # Sort by fraud rate
        high_fraud_merchants.sort(key=lambda x: x["fraud_rate"], reverse=True)
        for merchant_data in high_fraud_merchants[:5]:  # Top 5
            findings["evidence"].append(
                f"  - {merchant_data['merchant']}: {merchant_data['fraud_rate']*100:.1f}% fraud "
                f"({merchant_data['fraud_count']}/{merchant_data['total']} transactions)"
            )

    findings["analysis"]["merchant_fraud_correlation"] = {
        "high_fraud_merchants_count": len(high_fraud_merchants),
        "merchants_analyzed": len(merchant_fraud_data),
    }
