"""
Master Test Runner for Olorin Comprehensive Endpoint Testing Framework.

Executes all 7 phases of endpoint testing in sequence with proper reporting,
performance analysis, and comprehensive result summarization.

This master runner coordinates the entire test suite and provides:
- Sequential execution of all test phases
- Comprehensive performance analysis
- Detailed success/failure reporting
- Endpoint coverage verification
- Overall platform health assessment
"""

import pytest
import logging
import asyncio
import time
from datetime import datetime, timezone
from typing import Dict, Any, List

from .conftest import ENDPOINT_TEST_CONFIG, performance_metrics

logger = logging.getLogger(__name__)


class MasterTestRunner:
    """Master coordinator for comprehensive endpoint testing."""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.phase_results = {}
        self.overall_metrics = {
            "total_endpoints_tested": 0,
            "total_successful": 0,
            "total_failed": 0,
            "total_requests": 0,
            "average_response_time": 0.0,
            "slowest_endpoint": None,
            "fastest_endpoint": None,
            "coverage_by_category": {},
            "critical_issues": []
        }
    
    def record_phase_start(self, phase_name: str):
        """Record the start of a test phase."""
        logger.info("=" * 80)
        logger.info(f"STARTING PHASE: {phase_name}")
        logger.info("=" * 80)
        
        self.phase_results[phase_name] = {
            "start_time": time.time(),
            "status": "running",
            "endpoints_tested": 0,
            "successful": 0,
            "failed": 0,
            "errors": [],
            "warnings": []
        }
    
    def record_phase_end(self, phase_name: str, results: Dict[str, Any]):
        """Record the completion of a test phase."""
        if phase_name not in self.phase_results:
            return
        
        phase_data = self.phase_results[phase_name]
        phase_data.update({
            "end_time": time.time(),
            "duration": time.time() - phase_data["start_time"],
            "status": "completed",
            **results
        })
        
        logger.info("=" * 80)
        logger.info(f"COMPLETED PHASE: {phase_name}")
        logger.info(f"Duration: {phase_data['duration']:.1f} seconds")
        logger.info(f"Success Rate: {phase_data.get('successful', 0)}/{phase_data.get('endpoints_tested', 0)}")
        logger.info("=" * 80)
    
    def update_overall_metrics(self):
        """Update overall metrics based on phase results."""
        total_endpoints = 0
        total_successful = 0
        total_failed = 0
        
        for phase_name, phase_data in self.phase_results.items():
            if phase_data.get("status") == "completed":
                endpoints_tested = phase_data.get("endpoints_tested", 0)
                successful = phase_data.get("successful", 0)
                failed = phase_data.get("failed", 0)
                
                total_endpoints += endpoints_tested
                total_successful += successful
                total_failed += failed
        
        self.overall_metrics.update({
            "total_endpoints_tested": total_endpoints,
            "total_successful": total_successful,
            "total_failed": total_failed,
        })
        
        # Update from global performance metrics
        if performance_metrics["total_requests"] > 0:
            self.overall_metrics.update({
                "total_requests": performance_metrics["total_requests"],
                "average_response_time": (performance_metrics["total_response_time"] or 0) / max(1, performance_metrics["total_requests"] or 1),
                "slowest_endpoint": performance_metrics["slowest_endpoint"],
                "fastest_endpoint": performance_metrics["fastest_endpoint"]
            })
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive test execution report."""
        self.update_overall_metrics()
        
        # Calculate overall success rate
        total_endpoints = self.overall_metrics["total_endpoints_tested"]
        total_successful = self.overall_metrics["total_successful"]
        success_rate = (total_successful / total_endpoints * 100) if total_endpoints > 0 else 0
        
        # Categorize endpoints by functionality
        endpoint_categories = {
            "Health & Utility": ["GET /", "GET /health", "GET /health/full", "GET /version", "GET /favicon.ico", "GET /performance/health"],
            "Authentication": ["POST /auth/login", "POST /auth/login-json", "GET /auth/me", "POST /auth/logout"],
            "Investigation Management": ["POST /api/investigation", "GET /api/investigation/{id}", "PUT /api/investigation/{id}", "DELETE /api/investigation/{id}", "GET /api/investigations", "DELETE /api/investigation", "DELETE /api/investigations/delete_all"],
            "Analysis Engines": ["GET /api/device/{entity_id}", "GET /api/network/{entity_id}", "GET /api/location/source/oii/{user_id}", "GET /api/location/source/business/{user_id}", "GET /api/location/source/phone/{user_id}", "GET /api/location/risk-analysis/{user_id}", "GET /api/logs/{user_id}", "GET /api/oii/{user_id}"],
            "AI Agents": ["POST /v1/agent/invoke", "POST /v1/agent/start/{entity_id}"],
            "WebSocket": ["WS /ws/{investigation_id}"],
            "Error Handling": ["Various error scenarios"]
        }
        
        # Calculate coverage by category
        coverage_by_category = {}
        for category, endpoints in endpoint_categories.items():
            coverage_by_category[category] = {
                "total_endpoints": len(endpoints),
                "tested": 0,  # This would need to be calculated from actual test results
                "coverage_percentage": 0.0
            }
        
        report = {
            "execution_summary": {
                "start_time": datetime.fromtimestamp(self.start_time).isoformat() if self.start_time else None,
                "end_time": datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
                "total_duration_seconds": (self.end_time - self.start_time) if (self.start_time and self.end_time) else 0,
                "total_endpoints_tested": total_endpoints,
                "success_rate_percentage": success_rate,
                "total_requests_made": self.overall_metrics["total_requests"]
            },
            "phase_breakdown": {},
            "performance_analysis": {
                "average_response_time_ms": self.overall_metrics["average_response_time"],
                "slowest_endpoint": self.overall_metrics["slowest_endpoint"],
                "fastest_endpoint": self.overall_metrics["fastest_endpoint"],
                "requests_over_threshold": len([r for r in performance_metrics["requests"] if r["response_time_ms"] > 5000])
            },
            "endpoint_coverage": coverage_by_category,
            "critical_findings": self.overall_metrics["critical_issues"],
            "recommendations": self._generate_recommendations()
        }
        
        # Add phase breakdown
        for phase_name, phase_data in self.phase_results.items():
            report["phase_breakdown"][phase_name] = {
                "duration_seconds": phase_data.get("duration", 0),
                "endpoints_tested": phase_data.get("endpoints_tested", 0),
                "success_rate": (phase_data.get("successful", 0) / phase_data.get("endpoints_tested", 1)) * 100 if phase_data.get("endpoints_tested", 0) > 0 else 0,
                "status": phase_data.get("status", "unknown"),
                "errors": phase_data.get("errors", []),
                "warnings": phase_data.get("warnings", [])
            }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results."""
        recommendations = []
        
        # Performance recommendations
        if self.overall_metrics["average_response_time"] > 5000:
            recommendations.append("Consider optimizing API performance - average response time exceeds 5 seconds")
        
        # Success rate recommendations
        success_rate = (self.overall_metrics["total_successful"] / self.overall_metrics["total_endpoints_tested"] * 100) if self.overall_metrics["total_endpoints_tested"] > 0 else 0
        if success_rate < 90:
            recommendations.append("API reliability is below 90% - investigate failing endpoints")
        
        # Slow endpoint recommendations
        if performance_metrics["requests"]:
            slow_requests = [r for r in performance_metrics["requests"] if r["response_time_ms"] > 30000]
            if slow_requests:
                recommendations.append(f"Found {len(slow_requests)} requests taking over 30 seconds - consider timeout optimization")
        
        # Phase-specific recommendations
        for phase_name, phase_data in self.phase_results.items():
            if phase_data.get("status") != "completed":
                recommendations.append(f"Phase '{phase_name}' did not complete successfully - investigate failures")
            elif phase_data.get("errors"):
                recommendations.append(f"Phase '{phase_name}' has {len(phase_data['errors'])} errors - review error logs")
        
        if not recommendations:
            recommendations.append("All endpoint tests completed successfully - API is functioning well")
        
        return recommendations
    
    def print_executive_summary(self):
        """Print executive summary for stakeholders."""
        report = self.generate_comprehensive_report()
        
        logger.info("\n" + "=" * 100)
        logger.info("OLORIN ENDPOINT TESTING - EXECUTIVE SUMMARY")
        logger.info("=" * 100)
        
        # High-level metrics
        exec_summary = report["execution_summary"]
        logger.info(f"Test Execution Duration: {exec_summary['total_duration_seconds']:.1f} seconds")
        logger.info(f"Total Endpoints Tested: {exec_summary['total_endpoints_tested']}")
        logger.info(f"Overall Success Rate: {exec_summary['success_rate_percentage']:.1f}%")
        logger.info(f"Total API Requests: {exec_summary['total_requests_made']}")
        
        # Performance highlights
        perf = report["performance_analysis"]
        logger.info(f"Average Response Time: {perf['average_response_time_ms']:.1f}ms")
        
        if perf["slowest_endpoint"]:
            slowest = perf["slowest_endpoint"]
            logger.info(f"Slowest Endpoint: {slowest['method']} {slowest['endpoint']} ({slowest['response_time_ms']:.1f}ms)")
        
        if perf["fastest_endpoint"]:
            fastest = perf["fastest_endpoint"]
            logger.info(f"Fastest Endpoint: {fastest['method']} {fastest['endpoint']} ({fastest['response_time_ms']:.1f}ms)")
        
        # Phase results
        logger.info("\nPHASE RESULTS:")
        logger.info("-" * 50)
        for phase_name, phase_data in report["phase_breakdown"].items():
            status_icon = "âœ“" if phase_data["status"] == "completed" else "âœ—"
            logger.info(f"{status_icon} {phase_name}: {phase_data['success_rate']:.1f}% success ({phase_data['duration_seconds']:.1f}s)")
        
        # Critical findings
        if report["critical_findings"]:
            logger.info("\nCRITICAL FINDINGS:")
            logger.info("-" * 50)
            for finding in report["critical_findings"]:
                logger.warning(f"âš ï¸  {finding}")
        
        # Recommendations
        logger.info("\nRECOMMENDations:")
        logger.info("-" * 50)
        for rec in report["recommendations"]:
            logger.info(f"ðŸ’¡ {rec}")
        
        # Platform health assessment
        if exec_summary["success_rate_percentage"] >= 95:
            health_status = "EXCELLENT"
            health_color = "ðŸŸ¢"
        elif exec_summary["success_rate_percentage"] >= 90:
            health_status = "GOOD"
            health_color = "ðŸŸ¡"
        elif exec_summary["success_rate_percentage"] >= 75:
            health_status = "FAIR"
            health_color = "ðŸŸ "
        else:
            health_status = "POOR"
            health_color = "ðŸ”´"
        
        logger.info(f"\nPLATFORM HEALTH: {health_color} {health_status}")
        logger.info("=" * 100)


# Global test runner instance
master_runner = MasterTestRunner()


@pytest.mark.asyncio
async def test_comprehensive_endpoint_suite():
    """
    Master test that coordinates execution of all endpoint testing phases.
    This is the main entry point for the comprehensive endpoint test suite.
    """
    logger.info("\n" + "ðŸš€" * 20)
    logger.info("OLORIN COMPREHENSIVE ENDPOINT TESTING SUITE")
    logger.info("ðŸš€" * 20 + "\n")
    
    master_runner.start_time = time.time()
    
    try:
        # The actual phase tests are run via pytest discovery
        # This test serves as a coordinator and reporter
        
        # Record that the master suite has started
        logger.info(f"Target Server: {ENDPOINT_TEST_CONFIG['base_url']}")
        logger.info(f"Test Started: {datetime.now(timezone.utc).isoformat()}")
        logger.info(f"Performance Threshold: {ENDPOINT_TEST_CONFIG['performance_threshold_ms']}ms")
        
        # Log test configuration
        logger.info("\nTest Configuration:")
        for key, value in ENDPOINT_TEST_CONFIG.items():
            logger.info(f"  {key}: {value}")
        
        # This test will be run alongside all other tests
        # The actual coordination happens through pytest's natural execution
        
        # Simulate master coordination (actual coordination happens via pytest fixtures)
        await asyncio.sleep(0.1)  # Allow other tests to initialize
        
        logger.info("\nTest phases will be executed by pytest in the following order:")
        logger.info("1. Health & Utility Endpoints")
        logger.info("2. Authentication Endpoints")
        logger.info("3. Investigation Management Endpoints")
        logger.info("4. Analysis Endpoints")
        logger.info("5. Agent System Endpoints")
        logger.info("6. WebSocket Endpoints")
        logger.info("7. Error Handling Scenarios")
        
    except Exception as e:
        logger.error(f"Master test coordinator error: {str(e)}")
        raise
    
    finally:
        master_runner.end_time = time.time()


@pytest.fixture(scope="session", autouse=True)
def master_test_session_coordinator(request):
    """Session-scoped fixture that coordinates the entire test run."""
    
    def finalize_master_test_run():
        """Final cleanup and reporting."""
        master_runner.end_time = time.time()
        
        # Generate and print comprehensive report
        logger.info("\n" + "ðŸ“Š" * 20)
        logger.info("GENERATING COMPREHENSIVE TEST REPORT")
        logger.info("ðŸ“Š" * 20 + "\n")
        
        # Print executive summary
        master_runner.print_executive_summary()
        
        # Generate detailed report
        detailed_report = master_runner.generate_comprehensive_report()
        
        # Save report to file
        report_filename = f"endpoint_test_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.json"
        try:
            import json
            with open(f"tests/endpoint_test_reports/{report_filename}", "w") as f:
                json.dump(detailed_report, f, indent=2, default=str)
            logger.info(f"Detailed report saved to: {report_filename}")
        except Exception as e:
            logger.warning(f"Could not save detailed report: {e}")
        
        # Final statistics
        if performance_metrics["total_requests"] > 0:
            logger.info(f"\nFinal Statistics:")
            logger.info(f"Total API requests made: {performance_metrics['total_requests']}")
            logger.info(f"Total response time: {performance_metrics['total_response_time']:.1f}ms")
            logger.info(f"Average response time: {(performance_metrics['total_response_time'] or 0)/max(1, performance_metrics['total_requests'] or 1):.1f}ms")
    
    # Register finalizer
    request.addfinalizer(finalize_master_test_run)
    
    # Initialize master runner
    master_runner.start_time = time.time()
    logger.info("Master test session coordinator initialized")


if __name__ == "__main__":
    """Run the comprehensive endpoint test suite directly."""
    import sys
    import os
    
    # Add current directory to Python path
    sys.path.insert(0, os.path.dirname(__file__))
    
    # Run pytest with this test suite
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "--asyncio-mode=auto",
        f"--base-url={ENDPOINT_TEST_CONFIG['base_url']}"
    ])