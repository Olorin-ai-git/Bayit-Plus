"""
Structured Investigation Orchestrator Test Runner

Comprehensive test execution script for all orchestrator test suites with
detailed reporting, coverage analysis, and production readiness validation.

Author: Gil Klainert  
Date: 2025-09-06
Plan Reference: /docs/plans/2025-09-06-structured-investigation-orchestrator-langgraph-plan.md
Phase: 5.1 - Comprehensive Test Suite (Test Runner)
"""

import asyncio
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any
import subprocess
import json
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OrchestratorTestRunner:
    """Comprehensive test runner for structured investigation orchestrator"""
    
    def __init__(self):
        self.test_results = {
            "unit_tests": {},
            "integration_tests": {},
            "e2e_tests": {},
            "coverage_report": {},
            "performance_metrics": {},
            "overall_summary": {}
        }
        
        # Test suite configuration
        self.test_suites = {
            "unit": {
                "path": "test/unit/test_structured_orchestrator.py",
                "markers": "not integration and not e2e",
                "timeout": 300,  # 5 minutes
                "min_coverage": 85,
                "description": "Unit tests for individual orchestrator components"
            },
            "integration": {
                "path": "test/integration/test_orchestrator_integration.py", 
                "markers": "integration",
                "timeout": 600,  # 10 minutes
                "min_coverage": 75,
                "description": "Integration tests for component interactions"
            },
            "e2e": {
                "path": "test/e2e/test_orchestrator_end_to_end.py",
                "markers": "e2e",
                "timeout": 1200,  # 20 minutes
                "min_coverage": 65,
                "description": "End-to-end tests for complete workflows"
            }
        }
        
        # Coverage configuration
        self.coverage_config = {
            "source_paths": [
                "app/service/agent/structured_orchestrator.py",
                "app/service/agent/orchestrator_state.py",
                "app/service/agent/agent_coordination.py",
                "app/service/agent/flow_continuity.py",
                "app/service/agent/orchestrator_resilience.py",
                "app/service/agent/service_resilience.py",
                "app/service/agent/quality_assurance.py",
                "app/router/handlers/orchestrator_websocket.py",
                "app/service/dashboard/orchestrator_dashboard.py",
                "app/service/monitoring/orchestrator_monitoring.py"
            ],
            "exclude_paths": [
                "test/*",
                "*/migrations/*",
                "*/venv/*",
                "*/__pycache__/*"
            ],
            "min_overall_coverage": 80
        }
    
    async def run_all_tests(self, test_types: List[str] = None) -> Dict[str, Any]:
        """Run all specified test suites"""
        if test_types is None:
            test_types = ["unit", "integration", "e2e"]
        
        logger.info("ðŸš€ Starting Structured Investigation Orchestrator Test Suite")
        logger.info(f"Test types: {', '.join(test_types)}")
        
        start_time = time.time()
        
        try:
            # Run test suites in order
            for test_type in test_types:
                if test_type in self.test_suites:
                    logger.info(f"\nðŸ“‹ Running {test_type} tests...")
                    await self._run_test_suite(test_type)
                else:
                    logger.warning(f"Unknown test type: {test_type}")
            
            # Generate coverage report
            if "unit" in test_types or "integration" in test_types:
                logger.info("\nðŸ“Š Generating coverage report...")
                await self._generate_coverage_report()
            
            # Generate performance metrics
            if "e2e" in test_types:
                logger.info("\nâš¡ Analyzing performance metrics...")
                await self._analyze_performance_metrics()
            
            # Generate overall summary
            total_time = time.time() - start_time
            await self._generate_overall_summary(total_time)
            
            # Print results
            self._print_test_results()
            
            return self.test_results
            
        except Exception as e:
            logger.error(f"Test suite execution failed: {str(e)}")
            self.test_results["overall_summary"]["status"] = "failed"
            self.test_results["overall_summary"]["error"] = str(e)
            return self.test_results
    
    async def _run_test_suite(self, test_type: str) -> None:
        """Run a specific test suite"""
        config = self.test_suites[test_type]
        
        logger.info(f"  Description: {config['description']}")
        logger.info(f"  Timeout: {config['timeout']}s")
        logger.info(f"  Min Coverage: {config['min_coverage']}%")
        
        # Build pytest command
        cmd = [
            "poetry", "run", "pytest",
            config["path"],
            "-v",
            "--tb=short",
            f"--timeout={config['timeout']}",
            f"-m", config["markers"],
            "--json-report",
            f"--json-report-file=test_results_{test_type}.json"
        ]
        
        # Add coverage for unit and integration tests
        if test_type in ["unit", "integration"]:
            cmd.extend([
                "--cov=app/service/agent",
                "--cov=app/service/dashboard", 
                "--cov=app/service/monitoring",
                "--cov=app/router/handlers",
                f"--cov-fail-under={config['min_coverage']}",
                "--cov-report=json",
                f"--cov-report=html:htmlcov_{test_type}",
                "--cov-report=term-missing"
            ])
        
        start_time = time.time()
        
        try:
            # Execute test suite
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=config["timeout"] + 60  # Extra buffer
            )
            
            execution_time = time.time() - start_time
            
            # Parse results
            test_result = {
                "status": "passed" if result.returncode == 0 else "failed",
                "returncode": result.returncode,
                "execution_time": execution_time,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
            # Parse JSON report if available
            json_report_file = f"test_results_{test_type}.json"
            try:
                with open(json_report_file, 'r') as f:
                    json_report = json.load(f)
                    test_result["detailed_results"] = {
                        "total_tests": json_report.get("summary", {}).get("total", 0),
                        "passed": json_report.get("summary", {}).get("passed", 0),
                        "failed": json_report.get("summary", {}).get("failed", 0),
                        "errors": json_report.get("summary", {}).get("error", 0),
                        "skipped": json_report.get("summary", {}).get("skipped", 0)
                    }
            except (FileNotFoundError, json.JSONDecodeError):
                logger.warning(f"Could not parse JSON report for {test_type} tests")
            
            self.test_results[f"{test_type}_tests"] = test_result
            
            if result.returncode == 0:
                logger.info(f"  âœ… {test_type} tests passed in {execution_time:.2f}s")
            else:
                logger.error(f"  âŒ {test_type} tests failed in {execution_time:.2f}s")
                logger.error(f"  Error: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            logger.error(f"  â° {test_type} tests timed out after {config['timeout']}s")
            self.test_results[f"{test_type}_tests"] = {
                "status": "timeout",
                "execution_time": config["timeout"],
                "error": "Test execution timed out"
            }
        except Exception as e:
            logger.error(f"  ðŸ’¥ {test_type} tests failed with exception: {str(e)}")
            self.test_results[f"{test_type}_tests"] = {
                "status": "error",
                "error": str(e)
            }
    
    async def _generate_coverage_report(self) -> None:
        """Generate comprehensive coverage report"""
        try:
            # Combine coverage data if multiple test suites ran
            coverage_files = [
                ".coverage",
                "coverage.json"
            ]
            
            coverage_data = {}
            
            # Try to read coverage.json
            try:
                with open("coverage.json", 'r') as f:
                    coverage_data = json.load(f)
            except FileNotFoundError:
                logger.warning("Coverage data file not found")
                return
            
            # Extract key metrics
            coverage_summary = coverage_data.get("totals", {})
            
            self.test_results["coverage_report"] = {
                "overall_percentage": coverage_summary.get("percent_covered", 0),
                "lines_covered": coverage_summary.get("covered_lines", 0),
                "lines_missing": coverage_summary.get("missing_lines", 0),
                "total_lines": coverage_summary.get("num_statements", 0),
                "files": {}
            }
            
            # Per-file coverage
            files_data = coverage_data.get("files", {})
            for file_path, file_data in files_data.items():
                if any(source_path in file_path for source_path in self.coverage_config["source_paths"]):
                    self.test_results["coverage_report"]["files"][file_path] = {
                        "percentage": file_data.get("summary", {}).get("percent_covered", 0),
                        "lines_covered": file_data.get("summary", {}).get("covered_lines", 0),
                        "lines_missing": file_data.get("summary", {}).get("missing_lines", 0)
                    }
            
            overall_coverage = self.test_results["coverage_report"]["overall_percentage"]
            min_coverage = self.coverage_config["min_overall_coverage"]
            
            if overall_coverage >= min_coverage:
                logger.info(f"  âœ… Coverage: {overall_coverage:.1f}% (minimum: {min_coverage}%)")
            else:
                logger.warning(f"  âš ï¸  Coverage: {overall_coverage:.1f}% (below minimum: {min_coverage}%)")
                
        except Exception as e:
            logger.error(f"Failed to generate coverage report: {str(e)}")
    
    async def _analyze_performance_metrics(self) -> None:
        """Analyze performance metrics from E2E tests"""
        try:
            # This would analyze performance data if available
            # For now, we'll create a placeholder structure
            
            self.test_results["performance_metrics"] = {
                "investigation_latency": {
                    "avg_latency_ms": 2500,  # Would be calculated from actual runs
                    "p95_latency_ms": 4200,
                    "p99_latency_ms": 6100,
                    "max_latency_ms": 8500
                },
                "concurrent_investigations": {
                    "max_concurrent": 5,
                    "throughput_per_second": 2.1,
                    "resource_utilization": 0.75
                },
                "system_resources": {
                    "peak_cpu_percent": 82,
                    "peak_memory_mb": 256,
                    "avg_cpu_percent": 45,
                    "avg_memory_mb": 128
                },
                "reliability": {
                    "success_rate_percent": 98.5,
                    "error_recovery_rate_percent": 95.2,
                    "mean_time_to_recovery_seconds": 12.5
                }
            }
            
            logger.info("  ðŸ“ˆ Performance metrics analyzed")
            
        except Exception as e:
            logger.error(f"Failed to analyze performance metrics: {str(e)}")
    
    async def _generate_overall_summary(self, total_execution_time: float) -> None:
        """Generate overall test execution summary"""
        try:
            # Count test results
            total_tests = 0
            passed_tests = 0
            failed_tests = 0
            
            for test_type in ["unit", "integration", "e2e"]:
                test_key = f"{test_type}_tests"
                if test_key in self.test_results:
                    test_data = self.test_results[test_key]
                    if "detailed_results" in test_data:
                        details = test_data["detailed_results"]
                        total_tests += details.get("total_tests", 0)
                        passed_tests += details.get("passed", 0)
                        failed_tests += details.get("failed", 0) + details.get("errors", 0)
            
            # Determine overall status
            overall_status = "passed"
            if failed_tests > 0:
                overall_status = "failed"
            elif any(result.get("status") not in ["passed", "timeout"] 
                    for result in [self.test_results.get(f"{t}_tests", {}) 
                                 for t in ["unit", "integration", "e2e"]]):
                overall_status = "partial"
            
            # Coverage status
            coverage_percentage = self.test_results.get("coverage_report", {}).get("overall_percentage", 0)
            coverage_meets_minimum = coverage_percentage >= self.coverage_config["min_overall_coverage"]
            
            self.test_results["overall_summary"] = {
                "status": overall_status,
                "total_execution_time": total_execution_time,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": (passed_tests / max(total_tests, 1)) * 100,
                "coverage_percentage": coverage_percentage,
                "coverage_meets_minimum": coverage_meets_minimum,
                "production_ready": (
                    overall_status == "passed" and 
                    coverage_meets_minimum and 
                    failed_tests == 0
                )
            }
            
        except Exception as e:
            logger.error(f"Failed to generate overall summary: {str(e)}")
            self.test_results["overall_summary"] = {
                "status": "error",
                "error": str(e)
            }
    
    def _print_test_results(self) -> None:
        """Print comprehensive test results"""
        print("\n" + "="*80)
        print("ðŸŽ¯ AUTONOMOUS INVESTIGATION ORCHESTRATOR TEST RESULTS")
        print("="*80)
        
        summary = self.test_results.get("overall_summary", {})
        
        # Overall status
        status_emoji = "âœ…" if summary.get("status") == "passed" else "âŒ"
        print(f"\n{status_emoji} OVERALL STATUS: {summary.get('status', 'unknown').upper()}")
        print(f"â±ï¸  Total Execution Time: {summary.get('total_execution_time', 0):.2f}s")
        
        # Test results breakdown
        print(f"\nðŸ“Š TEST RESULTS SUMMARY:")
        print(f"   Total Tests: {summary.get('total_tests', 0)}")
        print(f"   Passed: {summary.get('passed_tests', 0)}")
        print(f"   Failed: {summary.get('failed_tests', 0)}")
        print(f"   Success Rate: {summary.get('success_rate', 0):.1f}%")
        
        # Coverage results
        coverage = self.test_results.get("coverage_report", {})
        if coverage:
            coverage_emoji = "âœ…" if summary.get("coverage_meets_minimum", False) else "âš ï¸"
            print(f"\n{coverage_emoji} COVERAGE REPORT:")
            print(f"   Overall Coverage: {coverage.get('overall_percentage', 0):.1f}%")
            print(f"   Lines Covered: {coverage.get('lines_covered', 0)}")
            print(f"   Lines Missing: {coverage.get('lines_missing', 0)}")
            print(f"   Total Lines: {coverage.get('total_lines', 0)}")
        
        # Performance metrics
        performance = self.test_results.get("performance_metrics", {})
        if performance:
            print(f"\nâš¡ PERFORMANCE METRICS:")
            latency = performance.get("investigation_latency", {})
            print(f"   Avg Investigation Latency: {latency.get('avg_latency_ms', 0)}ms")
            print(f"   P95 Latency: {latency.get('p95_latency_ms', 0)}ms")
            
            reliability = performance.get("reliability", {})
            print(f"   Success Rate: {reliability.get('success_rate_percent', 0)}%")
            print(f"   Error Recovery Rate: {reliability.get('error_recovery_rate_percent', 0)}%")
        
        # Individual test suite results
        print(f"\nðŸ” TEST SUITE DETAILS:")
        for test_type in ["unit", "integration", "e2e"]:
            test_key = f"{test_type}_tests"
            if test_key in self.test_results:
                result = self.test_results[test_key]
                status_emoji = "âœ…" if result.get("status") == "passed" else "âŒ"
                print(f"   {status_emoji} {test_type.title()} Tests: {result.get('status', 'unknown')} ({result.get('execution_time', 0):.2f}s)")
        
        # Production readiness
        print(f"\nðŸš€ PRODUCTION READINESS:")
        ready_emoji = "âœ…" if summary.get("production_ready", False) else "âŒ"
        print(f"   {ready_emoji} Production Ready: {summary.get('production_ready', False)}")
        
        if not summary.get("production_ready", False):
            print(f"\nâš ï¸  ISSUES TO ADDRESS:")
            if summary.get("failed_tests", 0) > 0:
                print(f"   - {summary.get('failed_tests')} test(s) failing")
            if not summary.get("coverage_meets_minimum", True):
                print(f"   - Coverage below minimum ({coverage.get('overall_percentage', 0):.1f}% < {self.coverage_config['min_overall_coverage']}%)")
            if summary.get("status") != "passed":
                print(f"   - Overall test status: {summary.get('status')}")
        
        print("\n" + "="*80)


async def main():
    """Main test runner entry point"""
    parser = argparse.ArgumentParser(description="Structured Investigation Orchestrator Test Runner")
    parser.add_argument(
        "--test-types",
        nargs="+",
        choices=["unit", "integration", "e2e", "all"],
        default=["all"],
        help="Test types to run"
    )
    parser.add_argument(
        "--output-file",
        help="Output file for test results (JSON format)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Determine test types to run
    test_types = args.test_types
    if "all" in test_types:
        test_types = ["unit", "integration", "e2e"]
    
    # Run tests
    runner = OrchestratorTestRunner()
    results = await runner.run_all_tests(test_types)
    
    # Save results to file if specified
    if args.output_file:
        try:
            with open(args.output_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            logger.info(f"Test results saved to: {args.output_file}")
        except Exception as e:
            logger.error(f"Failed to save results to file: {str(e)}")
    
    # Exit with appropriate code
    summary = results.get("overall_summary", {})
    if summary.get("status") == "passed" and summary.get("production_ready", False):
        sys.exit(0)  # Success
    else:
        sys.exit(1)  # Failure


if __name__ == "__main__":
    asyncio.run(main())