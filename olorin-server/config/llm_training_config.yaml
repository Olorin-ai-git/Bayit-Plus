# LLM Training Configuration
# All values use environment variable placeholders for configuration-driven design
# Feature: 026-llm-training-pipeline

llm_training:
  # Master switch for LLM training pipeline
  enabled: ${LLM_TRAINING_ENABLED:false}

  # LLM reasoning mode (replaces rule-based scoring when true)
  reasoning_enabled: ${LLM_REASONING_ENABLED:false}

  # Provider configuration
  provider:
    primary: ${LLM_TRAINING_PRIMARY_PROVIDER:claude}
    model_id: ${LLM_TRAINING_MODEL_ID:claude-3-5-sonnet-20241022}
    fallback_provider: ${LLM_TRAINING_FALLBACK_PROVIDER:openai}
    fallback_model_id: ${LLM_TRAINING_FALLBACK_MODEL_ID:gpt-4o-mini}

  # Batch processing settings
  batch_processing:
    batch_size: ${LLM_TRAINING_BATCH_SIZE:100}
    max_concurrent: ${LLM_TRAINING_MAX_CONCURRENT:5}
    timeout_seconds: ${LLM_TRAINING_TIMEOUT_SECONDS:120}
    retry_attempts: ${LLM_TRAINING_RETRY_ATTEMPTS:3}
    retry_delay_seconds: ${LLM_TRAINING_RETRY_DELAY_SECONDS:2}

  # Data sampling configuration
  data_sampling:
    fraud_ratio: ${LLM_TRAINING_FRAUD_RATIO:0.5}
    time_window_days: ${LLM_TRAINING_TIME_WINDOW_DAYS:365}
    min_sample_size: ${LLM_TRAINING_MIN_SAMPLE_SIZE:100}
    max_sample_size: ${LLM_TRAINING_MAX_SAMPLE_SIZE:10000}
    stratified_by_merchant: ${LLM_TRAINING_STRATIFIED_BY_MERCHANT:true}

  # Temporal holdout configuration (prevents data leakage)
  temporal_holdout:
    enabled: ${LLM_TEMPORAL_HOLDOUT_ENABLED:true}
    feature_period_months: ${LLM_FEATURE_PERIOD_MONTHS:6}
    observation_period_months: ${LLM_OBSERVATION_PERIOD_MONTHS:6}
    min_transactions_in_feature_period: ${LLM_MIN_TX_IN_FEATURE_PERIOD:2}

  # Scoring thresholds (defaults updated from 2025-12-15 training - best F1=30.3%)
  scoring:
    fraud_threshold: ${LLM_FRAUD_THRESHOLD:0.80}
    high_confidence_threshold: ${LLM_HIGH_CONFIDENCE_THRESHOLD:0.8}
    low_confidence_threshold: ${LLM_LOW_CONFIDENCE_THRESHOLD:0.2}
    require_reasoning: ${LLM_REQUIRE_REASONING:true}

  # Prompt configuration
  prompts:
    active_version: ${LLM_PROMPT_ACTIVE_VERSION:v14}
    prompts_directory: ${LLM_PROMPTS_DIRECTORY:config/prompts}
    cache_enabled: ${LLM_PROMPT_CACHE_ENABLED:true}
    cache_ttl_seconds: ${LLM_PROMPT_CACHE_TTL_SECONDS:3600}

  # Output configuration
  output:
    include_reasoning: ${LLM_OUTPUT_INCLUDE_REASONING:true}
    include_confidence: ${LLM_OUTPUT_INCLUDE_CONFIDENCE:true}
    include_feature_weights: ${LLM_OUTPUT_INCLUDE_FEATURE_WEIGHTS:true}
    max_reasoning_length: ${LLM_OUTPUT_MAX_REASONING_LENGTH:500}

  # Logging and metrics
  logging:
    log_prompts: ${LLM_LOG_PROMPTS:false}
    log_responses: ${LLM_LOG_RESPONSES:false}
    log_metrics: ${LLM_LOG_METRICS:true}
    metrics_export_enabled: ${LLM_METRICS_EXPORT_ENABLED:false}

  # Enhanced Feature Engineering (Enhancement 4)
  features:
    velocity:
      enabled: ${LLM_VELOCITY_FEATURES_ENABLED:true}
      windows_hours: ${LLM_VELOCITY_WINDOWS:[1,24,168,720]}
    lifecycle:
      enabled: ${LLM_LIFECYCLE_FEATURES_ENABLED:true}
    geo:
      enabled: ${LLM_GEO_FEATURES_ENABLED:true}
      impossible_travel_km: ${LLM_IMPOSSIBLE_TRAVEL_KM:500}
      impossible_travel_hours: ${LLM_IMPOSSIBLE_TRAVEL_HOURS:1}
    merchant:
      enabled: ${LLM_MERCHANT_FEATURES_ENABLED:true}
      peer_window_days: ${LLM_PEER_WINDOW_DAYS:30}

  # Dataset Management (Enhancement 1)
  dataset:
    target_fraud_entities: ${LLM_TARGET_FRAUD_ENTITIES:500}
    max_fraud_entities: ${LLM_MAX_FRAUD_ENTITIES:1000}
    legit_multiplier: ${LLM_LEGIT_MULTIPLIER:20}
    train_ratio: ${LLM_TRAIN_RATIO:0.70}
    validation_ratio: ${LLM_VALIDATION_RATIO:0.15}
    test_ratio: ${LLM_TEST_RATIO:0.15}
    stratify_by_merchant: ${LLM_STRATIFY_MERCHANT:true}
    min_samples_per_merchant: ${LLM_MIN_SAMPLES_MERCHANT:10}

  # Temporal Framework (Enhancement 2)
  temporal:
    rolling_windows_months: ${LLM_ROLLING_WINDOWS:[1,3,6,12]}
    ootv_enabled: ${LLM_OOTV_ENABLED:true}
    ootv_train_months: ${LLM_OOTV_TRAIN_MONTHS:9}
    ootv_eval_months: ${LLM_OOTV_EVAL_MONTHS:3}
    drift_psi_threshold: ${LLM_DRIFT_PSI_THRESHOLD:0.25}
    drift_check_interval_hours: ${LLM_DRIFT_CHECK_HOURS:24}

  # Classical Model & Calibration (Enhancement 3)
  classical_model:
    enabled: ${LLM_CLASSICAL_MODEL_ENABLED:true}
    model_type: ${LLM_MODEL_TYPE:logistic_regression}
    regularization: ${LLM_REGULARIZATION:l2}
    c_value: ${LLM_C_VALUE:1.0}
  calibration:
    method: ${LLM_CALIBRATION_METHOD:isotonic}
    min_samples: ${LLM_CALIBRATION_MIN_SAMPLES:100}
  threshold:
    cost_fn_ratio: ${LLM_COST_FN_RATIO:10}
    cost_fp_ratio: ${LLM_COST_FP_RATIO:1}
    search_steps: ${LLM_THRESHOLD_STEPS:100}

  # Hybrid Scoring (Enhancement 5) - weights from 2025-12-15 training (best F1)
  hybrid:
    mode: ${LLM_SCORING_MODE:hybrid}
    classical_weight: ${LLM_CLASSICAL_WEIGHT:0.65}
    llm_weight: ${LLM_LLM_WEIGHT:0.35}
    fallback_to_llm: ${LLM_FALLBACK_TO_LLM:true}
  explanation:
    enabled: ${LLM_EXPLANATION_ENABLED:true}
    max_length: ${LLM_EXPLANATION_MAX_LENGTH:500}
    include_contributions: ${LLM_INCLUDE_CONTRIBUTIONS:true}

  # Risk Bands (Enhancement 6) - aligned with 0.80 fraud threshold
  risk_bands:
    enabled: ${LLM_RISK_BANDS_ENABLED:true}
    auto_approve_max: ${LLM_BAND_APPROVE_MAX:0.60}
    log_only_max: ${LLM_BAND_LOG_MAX:0.80}
    review_max: ${LLM_BAND_REVIEW_MAX:0.90}
  segmentation:
    enabled: ${LLM_SEGMENTATION_ENABLED:true}
    config_path: ${LLM_SEGMENT_CONFIG:config/segment_thresholds.json}

  # Advanced Evaluation (Enhancement 7)
  evaluation:
    primary_metric: ${LLM_PRIMARY_METRIC:pr_auc}
    target_fpr: ${LLM_TARGET_FPR:0.005}
    cohort_dimensions: ${LLM_COHORT_DIMS:[merchant,region,device_type]}
  champion_challenger:
    enabled: ${LLM_CHAMPION_CHALLENGER:true}
    shadow_mode: ${LLM_SHADOW_MODE:true}
    min_samples: ${LLM_MIN_COMPARISON_SAMPLES:1000}
    promotion_threshold: ${LLM_PROMOTION_THRESHOLD:0.02}

  # Event-Driven Retraining (Enhancement 8)
  retraining:
    trigger_on_drift: ${LLM_TRIGGER_ON_DRIFT:true}
    trigger_on_degradation: ${LLM_TRIGGER_ON_DEGRADATION:true}
    degradation_threshold: ${LLM_DEGRADATION_THRESHOLD:0.05}
    cooldown_hours: ${LLM_RETRAIN_COOLDOWN_HOURS:24}
    require_regression_pass: ${LLM_REQUIRE_REGRESSION:true}
    min_regression_samples: ${LLM_MIN_REGRESSION_SAMPLES:100}
  versioning:
    snapshot_dir: ${LLM_SNAPSHOT_DIR:data/model_snapshots}
    max_snapshots: ${LLM_MAX_SNAPSHOTS:10}
