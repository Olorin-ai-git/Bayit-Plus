# Fraud Detection Prompt Templates v12
# Feature: 026-llm-training-pipeline
# ITERATION 9: Recall-focused with lower threshold and probabilistic scoring
#
# PROBLEM WITH v6-v11:
# 1. Missed 78% of fraud (14/18) - fraudsters look like normal users (1-2 devices)
# 2. High false positives - legitimate power users flagged (high device/IP counts)
#
# v12 Strategy:
# 1. LOWER threshold to 0.45 to catch more fraud (prioritize recall)
# 2. De-emphasize device/IP counts as primary signals (not discriminative)
# 3. Focus on transaction VALUE patterns and temporal anomalies
# 4. Use probabilistic reasoning - given base rate of 0.7% fraud, adjust prior

version: v12
created_at: "2024-12-12"
description: "Recall-focused prompt with probabilistic scoring and value-based signals"

# System prompt - RECALL FOCUSED with probabilistic reasoning
system_prompt: |
  You are an expert fraud detection analyst predicting FUTURE fraud risk.

  ## Critical Context
  - Base rate: ~0.7% of entities commit fraud (rare but costly)
  - COST ASYMMETRY: Missing fraud is 10x worse than false positives
  - Your goal: CATCH FRAUD even at cost of some false positives

  ## Why Previous Models Failed
  - Device/IP counts are NOT discriminative for fraud in this dataset
  - Fraudsters mimic normal users: 1-2 devices, single merchant focus
  - Legitimate power users have many devices/IPs but don't commit fraud

  ## Scoring System (Recall-Focused)

  **Baseline: 0.50** (neutral starting point)

  ### FRAUD SIGNALS (Add risk - focus on VALUE patterns):
  - First-time or new entity with limited history → +0.15 (fraud often hits new accounts)
  - Transaction value variance > avg * 1.5 → +0.15 (testing limits)
  - Few transactions (<5) with total GMV > $500 → +0.20 (large amounts early)
  - High avg transaction value (>$200) in first few transactions → +0.15
  - Multiple transactions in short burst → +0.10 (rapid-fire testing)

  ### LEGITIMATE SIGNALS (Subtract risk - be conservative):
  - Established history (>20 transactions) → -0.10 (proven track record)
  - Consistent transaction values over time → -0.05
  - Long account tenure with regular activity → -0.10

  ### KEY INSIGHT:
  Do NOT heavily penalize for high device/IP counts - these indicate
  legitimate power users more often than fraud in this dataset.

  ### Score Interpretation:
  - 0.00-0.35: Low risk
  - 0.35-0.45: Below threshold but monitor
  - 0.45-0.60: Elevated risk → PREDICT FRAUD
  - 0.60-1.00: High risk → PREDICT FRAUD

  ## Decision Threshold
  - risk_score >= 0.45 → predict FRAUD (lower threshold for recall)
  - risk_score < 0.45 → predict LEGITIMATE

  ## Output Format
  Provide JSON:
  ```json
  {
    "risk_score": <float 0.0-1.0>,
    "confidence": <float 0.0-1.0>,
    "prediction": "<FRAUD|LEGITIMATE>",
    "reasoning": "<brief explanation>",
    "key_indicators": [<list of signals that influenced the score>],
    "risk_factors": {
      "device_risk": <float 0.0-1.0>,
      "velocity_risk": <float 0.0-1.0>,
      "value_risk": <float 0.0-1.0>,
      "pattern_risk": <float 0.0-1.0>
    }
  }
  ```

# Main fraud analysis prompt
fraud_analysis_prompt: |
  Analyze this entity for FUTURE fraud risk.

  RECALL-FOCUSED SCORING (lower threshold = 0.45):
  1. Start at 0.50 (neutral)
  2. Focus on VALUE patterns and transaction history, not device counts
  3. Add risk for: new accounts, high values early, value variance
  4. Subtract risk for: established history, consistent patterns
  5. Predict FRAUD if final score >= 0.45

  ## Entity Information
  - Entity Type: {entity_type}
  - Entity Value: {entity_value}
  - Merchant: {merchant_name}

  ## Behavioral Features (Feature Period)
  - Total Transactions: {total_transactions}
  - Total GMV: ${total_gmv}
  - Average Transaction Value: ${avg_tx_value}
  - Transaction Value Std Dev: ${std_tx_value}
  - Unique Devices: {unique_devices}
  - Unique IPs: {unique_ips}
  - Unique Merchants: {unique_merchants}
  - Date Range: {date_range}

  ## Analysis Steps (VALUE-FOCUSED):

  1. **Assess account maturity**:
     - total_transactions < 5: Add +0.15 (new, unproven)
     - total_transactions > 20: Subtract -0.10 (established)

  2. **Check early value patterns**:
     - total_gmv > $500 with < 5 txs: Add +0.20 (large early)
     - avg_tx_value > $200: Add +0.15 (high value)
     - avg_tx_value < $50 and consistent: Subtract -0.05 (normal)

  3. **Check value variance**:
     - std_tx_value > avg_tx_value * 1.5: Add +0.15 (erratic)
     - std_tx_value < avg_tx_value * 0.3: Subtract -0.05 (stable)

  4. **NOTE ON DEVICES/IPs**:
     - Do NOT add heavy risk for high device/IP counts
     - These are not discriminative in this dataset
     - Only flag extreme cases (>10 devices AND <5 txs)

  FINAL: Sum all adjustments from baseline 0.50
  THRESHOLD: >= 0.45 = FRAUD

  Provide your analysis in JSON format:
  ```json
  {{
    "risk_score": <float 0.0-1.0>,
    "confidence": <float 0.0-1.0>,
    "prediction": "<FRAUD|LEGITIMATE>",
    "reasoning": "<explain which signals affected the score>",
    "key_indicators": [<list of detected signals>],
    "risk_factors": {{
      "device_risk": <float 0.0-1.0>,
      "velocity_risk": <float 0.0-1.0>,
      "value_risk": <float 0.0-1.0>,
      "pattern_risk": <float 0.0-1.0>
    }}
  }}
  ```

# Template for fraud indicators
fraud_indicators_template: |
  ## Behavioral Risk Indicators
  - Total Transactions: {total_transactions}
  - Total GMV: ${total_gmv}
  - Average Transaction Value: ${avg_tx_value}
  - Transaction Value Std Dev: ${std_tx_value}
  - Unique Devices: {device_count}
  - Unique IPs: {ip_count}
  - Unique Merchants: {merchant_count}

# Template for velocity analysis
velocity_analysis_template: |
  ## Transaction Velocity Patterns
  - Transaction Count: {total_transactions}
  - Average Transaction Value: ${avg_tx_value}
  - Value Variance (Std Dev): ${std_tx_value}
  - Activity Span: {first_tx_date} to {last_tx_date}

# Template for geographic analysis
geographic_analysis_template: |
  ## Network Diversity
  - Unique IP Addresses: {ip_count}
  - IP per Transaction Ratio: {ip_ratio}

# Template for device/network analysis
device_analysis_template: |
  ## Device Fingerprint Analysis
  - Unique Devices: {device_count}
  - Unique IPs: {ip_count}
  - Multi-Device Pattern: {multi_device}

# Template for historical patterns
historical_patterns_template: |
  ## Account History
  - First Transaction: {first_tx_date}
  - Last Transaction: {last_tx_date}
  - Transaction Count: {total_transactions}
  - Merchants Used: {merchant_count}

# Batch analysis prompt
batch_analysis_prompt: |
  Analyze {entity_count} entities for future fraud risk.

  Use recall-focused scoring (baseline 0.50, threshold 0.45).
  Focus on VALUE patterns, not device counts.

  {entities_data}

  Provide JSON array:
  ```json
  [
    {{
      "entity_id": "<entity identifier>",
      "risk_score": <float 0.0-1.0>,
      "confidence": <float 0.0-1.0>,
      "prediction": "<FRAUD|LEGITIMATE>",
      "reasoning": "<brief explanation>"
    }}
  ]
  ```

# Feedback analysis prompt
feedback_analysis_prompt: |
  The following prediction was incorrect:

  ## Original Prediction
  - Entity: {entity_value}
  - Predicted: {predicted_label}
  - Actual: {actual_label}
  - Risk Score: {risk_score}

  ## Entity Behavioral Data
  {entity_data}

  ## Actual Outcome
  - Committed Fraud: {committed_fraud}

  Analyze the error:
  ```json
  {{
    "error_type": "<false_positive|false_negative>",
    "likely_cause": "<explanation>",
    "missed_patterns": [<patterns that should have been weighted differently>],
    "suggested_adjustments": {{
      "signal_adjustments": {{<suggested weight changes>}},
      "threshold_adjustment": <suggested change>
    }}
  }}
  ```
