"""
Content Auditor Service
AI-powered content validation using Claude API for classification verification
"""
import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

import anthropic
from app.core.config import settings
from app.models.content import Content, Category
from app.models.librarian import LibrarianAction, ClassificationVerificationCache

logger = logging.getLogger(__name__)


@dataclass
class ClassificationVerification:
    """Result of a classification verification"""
    content_id: str
    fit_score: int  # 1-10
    is_correct: bool
    suggested_category: Optional[str] = None
    reasoning: Optional[str] = None


async def audit_content_items(
    content_ids: List[str],
    audit_id: str,
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    Audit content items for classification, metadata completeness, and quality.

    Args:
        content_ids: List of content IDs to audit
        audit_id: Parent audit report ID
        dry_run: If true, only report issues without fixing

    Returns:
        Dictionary with audit results
    """
    logger.info(f"ğŸ“š Auditing {len(content_ids)} content items...")

    results = {
        "status": "completed",
        "items_checked": len(content_ids),
        "missing_metadata": [],
        "misclassifications": [],
        "issues_found": 0,
    }

    if not content_ids:
        return results

    # Fetch all content items
    contents = await Content.find(
        {"_id": {"$in": content_ids}}
    ).to_list(length=None)

    logger.info(f"   Found {len(contents)} content items in database")

    # Check metadata completeness
    logger.info("   Checking metadata completeness...")
    missing_metadata = await check_metadata_completeness(contents)
    results["missing_metadata"] = missing_metadata

    # Verify classifications with AI
    logger.info("   Verifying classifications with Claude AI...")
    misclassifications = await verify_classifications(contents, audit_id)
    results["misclassifications"] = misclassifications

    # Auto-fix if not dry run
    if not dry_run and (missing_metadata or misclassifications):
        logger.info("   Auto-fixing issues...")
        from app.services.auto_fixer import fix_content_issues
        await fix_content_issues(missing_metadata, misclassifications, audit_id)

    results["issues_found"] = len(missing_metadata) + len(misclassifications)

    logger.info(f"   âœ… Content audit complete: {results['issues_found']} issues found")
    return results


async def check_metadata_completeness(contents: List[Content]) -> List[Dict[str, Any]]:
    """
    Check for missing or incomplete metadata.

    Checks:
    - Missing thumbnail/backdrop
    - Missing TMDB/IMDB data
    - Missing description
    - Empty cast/director for movies
    """
    missing_metadata = []

    for content in contents:
        issues = []

        if not content.thumbnail:
            issues.append("missing_thumbnail")
        if not content.backdrop:
            issues.append("missing_backdrop")
        if not content.tmdb_id:
            issues.append("missing_tmdb_id")
        if not content.imdb_id and not content.is_series:  # Movies should have IMDB
            issues.append("missing_imdb_id")
        if not content.description or len(content.description) < 20:
            issues.append("incomplete_description")
        if not content.genre:
            issues.append("missing_genre")

        if issues:
            missing_metadata.append({
                "content_id": str(content.id),
                "title": content.title,
                "issues": issues,
                "fixable": True,  # Can use TMDB to fix
            })

    logger.info(f"      Found {len(missing_metadata)} items with missing metadata")
    return missing_metadata


async def verify_classifications(
    contents: List[Content],
    audit_id: str
) -> List[Dict[str, Any]]:
    """
    Verify content classifications using Claude AI.

    Uses batch processing and caching to optimize API calls.
    """
    misclassifications = []

    # Group content by category for efficient batch processing
    category_groups: Dict[str, List[Content]] = {}
    for content in contents:
        if content.category_id not in category_groups:
            category_groups[content.category_id] = []
        category_groups[content.category_id].append(content)

    # Fetch all categories
    categories_dict = {}
    all_categories = await Category.find({}).to_list(length=None)
    for cat in all_categories:
        categories_dict[str(cat.id)] = cat

    # Verify each category group
    for category_id, group_contents in category_groups.items():
        category = categories_dict.get(category_id)
        if not category:
            logger.warning(f"âš ï¸ Category {category_id} not found")
            continue

        logger.info(f"      Verifying {len(group_contents)} items in category: {category.name}")

        # Check cache first
        cached_results, uncached_contents = await get_cached_verifications(
            group_contents, category_id
        )
        misclassifications.extend(cached_results)

        if not uncached_contents:
            continue

        # Batch process uncached items (50 at a time)
        batch_size = 50
        for i in range(0, len(uncached_contents), batch_size):
            batch = uncached_contents[i:i + batch_size]

            try:
                verifications = await verify_classification_batch(batch, category)

                # Process results
                for verification in verifications:
                    # Cache result
                    await cache_verification(verification, category_id)

                    # Add to misclassifications if needed
                    if not verification.is_correct and verification.fit_score < 6:
                        misclassifications.append({
                            "content_id": verification.content_id,
                            "current_category": category.name,
                            "suggested_category": verification.suggested_category,
                            "fit_score": verification.fit_score,
                            "reasoning": verification.reasoning,
                            "confidence": (10 - verification.fit_score) / 10.0,  # Inverse
                        })

            except Exception as e:
                logger.error(f"âŒ Failed to verify batch: {e}")

            # Rate limiting
            await asyncio.sleep(0.5)

    logger.info(f"      Found {len(misclassifications)} potential misclassifications")
    return misclassifications


async def get_cached_verifications(
    contents: List[Content],
    category_id: str
) -> tuple[List[Dict[str, Any]], List[Content]]:
    """
    Check cache for existing verification results.

    Returns: (cached_misclassifications, uncached_contents)
    """
    cached_misclass = []
    uncached = []

    seven_days_ago = datetime.utcnow() - timedelta(days=7)

    for content in contents:
        # Check if we have a recent verification
        cached = await ClassificationVerificationCache.find_one({
            "content_id": str(content.id),
            "category_id": category_id,
            "last_verified": {"$gte": seven_days_ago}
        })

        if cached:
            # Use cached result
            if not cached.is_correct and cached.fit_score < 6:
                cached_misclass.append({
                    "content_id": str(content.id),
                    "current_category": cached.category_name,
                    "suggested_category": cached.suggested_category_name,
                    "fit_score": cached.fit_score,
                    "reasoning": cached.reasoning,
                    "confidence": (10 - cached.fit_score) / 10.0,
                })
        else:
            uncached.append(content)

    return cached_misclass, uncached


async def verify_classification_batch(
    items: List[Content],
    category: Category
) -> List[ClassificationVerification]:
    """
    Batch verify if content items belong in their category using Claude AI.

    Processes up to 50 items per API call for cost efficiency.
    """
    if not items:
        return []

    # Prepare items for Claude
    items_data = []
    for item in items:
        items_data.append({
            "content_id": str(item.id),
            "title": item.title,
            "description": item.description[:200] if item.description else "",
            "genre": item.genre,
            "year": item.year,
            "is_series": item.is_series,
        })

    # Build prompt (Hebrew first, platform language)
    prompt = f"""××ª×” ×¡×¤×¨×Ÿ AI ×œ××¢×¨×›×ª Bayit+, ×¤×œ×˜×¤×•×¨××ª ×¡×˜×¨×™××™× ×’ ×™×©×¨××œ×™×ª.

×‘×“×•×§ ×”×× {len(items)} ×¤×¨×™×˜×™ ×”×ª×•×›×Ÿ ×”×‘××™× ××¡×•×•×’×™× × ×›×•×Ÿ ×‘×§×˜×’×•×¨×™×”.

**×§×˜×’×•×¨×™×” × ×•×›×—×™×ª:**
×©×: {category.name}
×©× ×‘×× ×’×œ×™×ª: {category.name_en or ""}
×ª×™××•×¨: {category.description or "×œ× ×–××™×Ÿ"}

**×¤×¨×™×˜×™ ×ª×•×›×Ÿ ×œ×‘×“×™×§×”:**
{json.dumps(items_data, ensure_ascii=False, indent=2)}

**×”×•×¨××•×ª:**
×¢×‘×•×¨ ×›×œ ×¤×¨×™×˜, ×”×¢×¨×š:
1. **×¦×™×•×Ÿ ×”×ª×××”** (1-10): ×¢×“ ×›××” ×”×¤×¨×™×˜ ××ª××™× ×œ×§×˜×’×•×¨×™×”
   - 1-3: ×œ× ××ª××™× ×‘×›×œ×œ, ×“×•×¨×© ×©×™× ×•×™ ×§×˜×’×•×¨×™×”
   - 4-6: ××ª××™× ×‘××•×¤×Ÿ ×—×œ×§×™, ×›× ×¨××” ×™×© ×§×˜×’×•×¨×™×” ×˜×•×‘×” ×™×•×ª×¨
   - 7-8: ××ª××™× ×”×™×˜×‘
   - 9-10: ××ª××™× ×‘××•×¤×Ÿ ××•×©×œ×

2. **×”×× ×”×¡×™×•×•×’ × ×›×•×Ÿ?** true/false

3. **×§×˜×’×•×¨×™×” ××•×¦×¢×ª**: ×× ×”×¦×™×•×Ÿ × ××•×š ×-7, ×”×¦×¢ ×§×˜×’×•×¨×™×” ×˜×•×‘×” ×™×•×ª×¨

4. **×”×¡×‘×¨ ×§×¦×¨**: × ××§ ××ª ×”×”×—×œ×˜×”

**×”×—×–×¨ JSON ×‘×¤×•×¨××˜ ×”×‘×:**
{{
    "verifications": [
        {{
            "content_id": "...",
            "fit_score": 8,
            "is_correct": true,
            "suggested_category": null,
            "reasoning": "×¡×¨×˜ ×“×¨××” ×™×©×¨××œ×™, ××ª××™× ×œ×§×˜×’×•×¨×™×™×ª ×¡×¨×˜×™×"
        }}
    ]
}}

×©×™× ×œ×‘: ×”×ª×©×•×‘×” ×¦×¨×™×›×” ×œ×”×™×•×ª **×¨×§** JSON, ×œ×œ× ×˜×§×¡×˜ × ×•×¡×£."""

    try:
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        # Parse JSON response
        response_text = response.content[0].text.strip()

        # Clean up response if needed
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]

        data = json.loads(response_text.strip())

        # Convert to ClassificationVerification objects
        verifications = []
        for v in data.get("verifications", []):
            verifications.append(ClassificationVerification(
                content_id=v.get("content_id", ""),
                fit_score=v.get("fit_score", 5),
                is_correct=v.get("is_correct", True),
                suggested_category=v.get("suggested_category"),
                reasoning=v.get("reasoning", ""),
            ))

        return verifications

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse Claude response: {e}")
        logger.error(f"Response text: {response_text}")
        return []
    except Exception as e:
        logger.error(f"Error calling Claude API: {e}")
        return []


async def cache_verification(verification: ClassificationVerification, category_id: str):
    """Cache a verification result"""
    try:
        # Get content for additional info
        from beanie import PydanticObjectId
        content = await Content.get(PydanticObjectId(verification.content_id))
        if not content:
            return

        # Get category
        category = await Category.get(PydanticObjectId(category_id))

        cache_entry = ClassificationVerificationCache(
            content_id=verification.content_id,
            category_id=category_id,
            fit_score=verification.fit_score,
            is_correct=verification.is_correct,
            suggested_category_id=None,  # Would need lookup
            suggested_category_name=verification.suggested_category,
            reasoning=verification.reasoning,
            content_title=content.title,
            content_genre=content.genre,
            category_name=category.name if category else None,
            last_verified=datetime.utcnow(),
            expires_at=datetime.utcnow() + timedelta(days=7),
        )

        await cache_entry.insert()

    except Exception as e:
        logger.warning(f"Failed to cache verification: {e}")


async def generate_ai_insights(audit_report) -> List[str]:
    """
    Generate AI-powered insights and recommendations from audit results.

    Uses Claude to analyze patterns and provide actionable recommendations.
    """
    insights = []

    try:
        # Prepare audit summary for Claude
        summary_data = {
            "total_items": audit_report.summary.get("total_items", 0),
            "issues_found": audit_report.summary.get("issues_found", 0),
            "issues_fixed": audit_report.summary.get("issues_fixed", 0),
            "broken_streams_count": len(audit_report.broken_streams),
            "missing_metadata_count": len(audit_report.missing_metadata),
            "misclassifications_count": len(audit_report.misclassifications),
            "orphaned_items_count": len(audit_report.orphaned_items),
        }

        # Sample some issues for context
        sample_issues = {
            "broken_streams": audit_report.broken_streams[:5],
            "missing_metadata": audit_report.missing_metadata[:5],
            "misclassifications": audit_report.misclassifications[:3],
        }

        prompt = f"""××ª×” ×¡×¤×¨×Ÿ AI ×œ××¢×¨×›×ª Bayit+. × ×ª×— ××ª ×ª×•×¦××•×ª ×”×‘×™×§×•×¨×ª ×”×‘××•×ª ×•×–×”×” ×“×¤×•×¡×™× ×•×” recommendations.

**×¡×™×›×•× ×‘×™×§×•×¨×ª:**
{json.dumps(summary_data, ensure_ascii=False, indent=2)}

**×“×•×’×××•×ª ×‘×¢×™×•×ª:**
{json.dumps(sample_issues, ensure_ascii=False, indent=2)}

**×”×•×¨××•×ª:**
1. ×–×”×” ×“×¤×•×¡×™× ××¢×¨×›×ª×™×™× (×œ××©×œ: ×›×œ ×”×ª×•×›×Ÿ ×××§×•×¨ X ×©×‘×•×¨)
2. ×”×¦×¢ ×”××œ×¦×•×ª ×œ×× ×™×¢×ª ×‘×¢×™×•×ª ×¢×ª×™×“×™×•×ª
3. ×“×¨×’ ×‘×¢×™×•×ª ×œ×¤×™ ×—×•××¨×”
4. ×–×”×” ×”×–×“×× ×•×™×•×ª ×œ×©×™×¤×•×¨ ××™×›×•×ª ×”××˜××“××˜×”

×”×—×–×¨ ×¨×©×™××ª ×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª (3-5 ×¤×¨×™×˜×™×), ×›×œ ×¤×¨×™×˜ ××©×¤×˜ ××—×“ ×‘×¨×•×¨.
×”×—×–×¨ JSON:
{{
    "insights": [
        "×ª×•×‘× ×” 1...",
        "×ª×•×‘× ×” 2...",
        "×”××œ×¦×” 1..."
    ]
}}"""

        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1000,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        response_text = response.content[0].text.strip()

        # Clean up response
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]

        data = json.loads(response_text.strip())
        insights = data.get("insights", [])

    except Exception as e:
        logger.warning(f"Failed to generate AI insights: {e}")
        insights = []

    return insights
