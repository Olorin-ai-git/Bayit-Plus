# Mock Data Enforcement CI/CD Pipeline
# Author: Gil Klainert
# Created: 2025-01-08
# Version: 1.0.0
#
# This workflow enforces zero-tolerance mock data policies across all commits,
# pull requests, and deployments with comprehensive reporting and alerting.

name: ðŸš¨ Mock Data Enforcement Pipeline

on:
  # Trigger on all push events
  push:
    branches: ['*']
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
      - 'LICENSE'
  
  # Trigger on all pull requests
  pull_request:
    branches: ['main', 'develop', 'staging']
    types: [opened, synchronize, reopened, ready_for_review]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
      - 'LICENSE'
  
  # Manual trigger for audits
  workflow_dispatch:
    inputs:
      scan_type:
        description: 'Scan Type'
        required: true
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'staged'
          - 'audit'
          - 'emergency'
      fail_threshold:
        description: 'Failure Threshold'
        required: true
        default: 'HIGH'
        type: choice
        options:
          - 'CRITICAL'
          - 'HIGH'
          - 'MEDIUM'
          - 'LOW'
      generate_report:
        description: 'Generate Detailed Report'
        required: false
        default: true
        type: boolean

  # Scheduled weekly audit
  schedule:
    - cron: '0 6 * * 1'  # Every Monday at 6 AM UTC

# Workflow-level permissions
permissions:
  contents: read
  pull-requests: write
  checks: write
  actions: read
  security-events: write
  statuses: write

# Global environment variables
env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  POETRY_VERSION: '1.7.1'
  PRE_COMMIT_VERSION: '3.6.0'
  
  # Mock detection configuration
  MOCK_DETECTION_CONFIG: 'scripts/git-hooks/mock-detection-config.yml'
  MOCK_DETECTION_FAIL_THRESHOLD: 'HIGH'
  MOCK_DETECTION_TIMEOUT: '600'  # 10 minutes
  
  # Performance settings
  PARALLEL_WORKERS: '4'
  MAX_FILE_SIZE_MB: '10'
  
  # Reporting settings
  GENERATE_JSON_REPORT: 'true'
  GENERATE_HTML_REPORT: 'true'
  UPLOAD_ARTIFACTS: 'true'

jobs:
  # ============================================================================
  # Pre-flight Checks
  # ============================================================================
  preflight:
    name: ðŸ›« Pre-flight System Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      system-ready: ${{ steps.validation.outputs.ready }}
      config-valid: ${{ steps.validation.outputs.config-valid }}
      skip-enforcement: ${{ steps.validation.outputs.skip-enforcement }}
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comprehensive analysis
      
      - name: ðŸ Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ”§ Install Core Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml pathspec jsonschema
      
      - name: âœ… Validate System Components
        id: validation
        run: |
          echo "ðŸ” Validating mock detection system..."
          
          # Check for required files
          REQUIRED_FILES=(
            "scripts/git-hooks/detect-mock-data.py"
            "scripts/git-hooks/mock-detection-config.yml"
            ".pre-commit-config.yaml"
          )
          
          MISSING_FILES=()
          for file in "${REQUIRED_FILES[@]}"; do
            if [[ ! -f "$file" ]]; then
              MISSING_FILES+=("$file")
            fi
          done
          
          if [[ ${#MISSING_FILES[@]} -gt 0 ]]; then
            echo "âŒ Missing required files:"
            printf '%s\n' "${MISSING_FILES[@]}"
            echo "ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Validate configuration
          if python -c "import yaml; yaml.safe_load(open('${{ env.MOCK_DETECTION_CONFIG }}'))"; then
            echo "âœ… Configuration file is valid"
            echo "config-valid=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Invalid configuration file"
            echo "config-valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check for emergency bypass
          if [[ "${{ github.event.head_commit.message }}" == *"[EMERGENCY-BYPASS]"* ]] && \
             [[ "${{ github.actor }}" == "gil-klainert" || "${{ github.actor }}" == "security-admin" ]]; then
            echo "âš ï¸  Emergency bypass detected - authorized user"
            echo "skip-enforcement=true" >> $GITHUB_OUTPUT
          else
            echo "skip-enforcement=false" >> $GITHUB_OUTPUT
          fi
          
          echo "ready=true" >> $GITHUB_OUTPUT
          echo "ðŸŽ‰ System validation completed successfully"

  # ============================================================================
  # Mock Data Detection and Enforcement
  # ============================================================================
  mock-data-detection:
    name: ðŸš¨ Mock Data Detection & Enforcement
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.system-ready == 'true' && needs.preflight.outputs.skip-enforcement == 'false'
    timeout-minutes: 15
    
    strategy:
      matrix:
        scan-mode: [
          { name: 'critical-only', threshold: 'CRITICAL', fast: true },
          { name: 'comprehensive', threshold: 'HIGH', fast: false }
        ]
    
    outputs:
      violations-found: ${{ steps.detection.outputs.violations-found }}
      critical-violations: ${{ steps.detection.outputs.critical-violations }}
      scan-summary: ${{ steps.detection.outputs.scan-summary }}
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      
      - name: ðŸ Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Detection Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml pathspec jsonschema
          
          # Install additional analysis dependencies
          pip install pandas matplotlib seaborn  # For reporting
          pip install safety bandit  # For security analysis
      
      - name: ðŸ” Execute Mock Data Detection
        id: detection
        env:
          SCAN_MODE: ${{ matrix.scan-mode.name }}
          FAIL_THRESHOLD: ${{ matrix.scan-mode.threshold }}
          FAST_MODE: ${{ matrix.scan-mode.fast }}
        run: |
          echo "ðŸš€ Starting mock data detection scan..."
          echo "ðŸ“‹ Scan mode: $SCAN_MODE"
          echo "ðŸŽ¯ Threshold: $FAIL_THRESHOLD"
          
          # Determine scan arguments
          SCAN_ARGS=(
            "--fail-on" "$FAIL_THRESHOLD"
            "--config" "${{ env.MOCK_DETECTION_CONFIG }}"
            "--output-json" "mock-detection-${SCAN_MODE}.json"
          )
          
          if [[ "$FAST_MODE" == "true" ]]; then
            SCAN_ARGS+=("--staged")
          else
            SCAN_ARGS+=("--directory" ".")
          fi
          
          if [[ "${{ github.event_name }}" != "pull_request" ]]; then
            SCAN_ARGS+=("--verbose")
          fi
          
          # Execute detection
          EXIT_CODE=0
          python scripts/git-hooks/detect-mock-data.py "${SCAN_ARGS[@]}" || EXIT_CODE=$?
          
          # Process results
          if [[ -f "mock-detection-${SCAN_MODE}.json" ]]; then
            TOTAL_VIOLATIONS=$(python -c "import json; f=open('mock-detection-${SCAN_MODE}.json'); data=json.load(f); print(data['scan_metadata']['total_violations'])")
            
            CRITICAL_VIOLATIONS=$(python -c "import json; f=open('mock-detection-${SCAN_MODE}.json'); data=json.load(f); print(data['summary_by_severity']['CRITICAL'])")
            
            echo "violations-found=$Page_Down" >> $GITHUB_OUTPUT
            echo "critical-violations=$CRITICAL_VIOLATIONS" >> $GITHUB_OUTPUT
            
            # Create scan summary
            SCAN_SUMMARY="Scan: $SCAN_MODE | Violations: $TOTAL_VIOLATIONS | Critical: $CRITICAL_VIOLATIONS"
            echo "scan-summary=$SCAN_SUMMARY" >> $GITHUB_OUTPUT
            
            if [[ $TOTAL_VIOLATIONS -gt 0 ]]; then
              echo "ðŸš¨ Found $TOTAL_VIOLATIONS violations ($CRITICAL_VIOLATIONS critical)"
              echo "::warning title=Mock Data Violations::Found $TOTAL_VIOLATIONS mock data violations"
            else
              echo "âœ… No violations found in $SCAN_MODE scan"
            fi
          else
            echo "âŒ Detection report not generated"
            exit 1
          fi
          
          # Set step status
          if [[ $EXIT_CODE -ne 0 ]] && [[ $TOTAL_VIOLATIONS -gt 0 ]]; then
            echo "::error title=Mock Data Policy Violation::Critical mock data violations found - blocking deploymPage_DownPage_Upent"
            exit $EXIT_CODE
          fi
      
      - name: ðŸ“Š Generate Visual Reports
        if: always() && env.GENERATE_HTML_REPORT == 'true'
        run: |
          echo "ðŸ“ˆ Generating visual reports..."
          
          python -c "
import json
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Load detection results
report_files = list(Path('.').glob('mock-detection-*.json'))
if not report_files:
    print('No report files found')
    exit()

for report_file in report_files:
    with open(report_file) as f:
        data = json.load(f)
    
    # Create severity distribution chart
    severity_data = data['summary_by_severity']
    
    plt.figure(figsize=(10, 6))
    plt.bar(severity_data.keys(), severity_data.values(), 
            color=['red', 'orange', 'yellow', 'blue'])
    plt.title(f'Mock Data Violations by Severity - {report_file.stem}')
    plt.xlabel('Severity Level')
    plt.ylabel('Number of Violations')
    plt.savefig(f'{report_file.stem}-severity-chart.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'Generated chart: {report_file.stem}-severity-chart.png')
          "
      
      - name: ðŸ“¤ Upload Detection Artifacts
        if: always() && env.UPLOAD_ARTIFACTS == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: mock-detection-results-${{ matrix.scan-mode.name }}-${{ github.run_number }}
          path: |
            mock-detection-*.json
            *-severity-chart.png
            .mock-detection.log
          retention-days: 30
      
      - name: ðŸ·ï¸ Tag Scan Results
        if: always()
        run: |
          echo "ðŸ·ï¸ Tagging scan results for downstream jobs..."
          echo "SCAN_MODE=${{ matrix.scan-mode.name }}" >> $GITHUB_ENV
          echo "VIOLATIONS_FOUND=${{ steps.detection.outputs.violations-found }}" >> $GITHUB_ENV
          echo "CRITICAL_VIOLATIONS=${{ steps.detection.outputs.critical-violations }}" >> $GITHUB_ENV

  # ============================================================================
  # Pull Request Integration and Reporting
  # ============================================================================
  pr-integration:
    name: ðŸ“‹ Pull Request Integration
    runs-on: ubuntu-latest
    needs: [preflight, mock-data-detection]
    if: github.event_name == 'pull_request'
    timeout-minutes: 5
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸ“Š Download Detection Results
        uses: actions/download-artifact@v4
        with:
          pattern: mock-detection-results-*
          merge-multiple: true
      
      - name: ðŸ’¬ Post PR Comment
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Find all detection result files
            const reportFiles = fs.readdirSync('.')
              .filter(file => file.startsWith('mock-detection-') && file.endsWith('.json'));
            
            if (reportFiles.length === 0) {
              console.log('No detection results found');
              return;
            }
            
            let comment = '## ðŸš¨ Mock Data Detection Results\n\n';
            let hasViolations = false;
            let totalViolations = 0;
            let criticalViolations = 0;
            
            for (const reportFile of reportFiles) {
              try {
                const data = JSON.parse(fs.readFileSync(reportFile, 'utf8'));
                const scanMode = reportFile.replace('mock-detection-', '').replace('.json', '');
                
                const violations = data.scan_metadata.total_violations;
                const critical = data.summary_by_severity.CRITICAL;
                const high = data.summary_by_severity.HIGH;
                const medium = data.summary_by_severity.MEDIUM;
                const low = data.summary_by_severity.LOW;
                
                totalViolations += violations;
                criticalViolations += critical;
                
                if (violations > 0) {
                  hasViolations = true;
                  comment += `### âŒ ${scanMode.toUpperCase()} Scan\n`;
                  comment += `- **Total Violations**: ${violations}\n`;
                  comment += `- **Critical**: ${critical} ðŸ”´\n`;
                  comment += `- **High**: ${high} ðŸŸ \n`;
                  comment += `- **Medium**: ${medium} ðŸŸ¡\n`;
                  comment += `- **Low**: ${low} ðŸ”µ\n\n`;
                  
                  // Show top violations
                  const topViolations = data.detailed_violations
                    .filter(v => ['CRITICAL', 'HIGH'].includes(v.severity))
                    .slice(0, 5);
                  
                  if (topViolations.length > 0) {
                    comment += `#### Top Critical/High Violations:\n`;
                    for (const violation of topViolations) {
                      comment += `- \`${violation.file_path}:${violation.line_number}\` - ${violation.pattern_type}\n`;
                    }
                    comment += '\n';
                  }
                } else {
                  comment += `### âœ… ${scanMode.toUpperCase()} Scan - No Violations\n\n`;
                }
                
              } catch (error) {
                console.error(`Error processing ${reportFile}:`, error);
              }
            }
            
            // Add summary and recommendations
            if (hasViolations) {
              comment += '## ðŸ› ï¸ Action Required\n\n';
              if (criticalViolations > 0) {
                comment += 'âš ï¸ **CRITICAL VIOLATIONS FOUND** - This PR will be blocked until all critical mock data violations are resolved.\n\n';
              }
              comment += 'Please review and remove all mock/fake/placeholder data from your code. ';
              comment += 'Refer to the [Mock Data Detection Guide](docs/hooks/mock-detection-guide.md) for assistance.\n\n';
            } else {
              comment += '## ðŸŽ‰ All Clear!\n\nNo mock data violations detected. Great job maintaining clean code standards!\n\n';
            }
            
            comment += `---\n*ðŸ¤– Automated by Mock Data Detection System v1.0.0*`;
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: âœ… Set PR Status Check
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const totalViolations = ${{ needs.mock-data-detection.outputs.violations-found || 0 }};
            const criticalViolations = ${{ needs.mock-data-detection.outputs.critical-violations || 0 }};
            
            const state = totalViolations === 0 ? 'success' : 'failure';
            const description = totalViolations === 0 
              ? 'No mock data violations detected'
              : `${totalViolations} violations found (${criticalViolations} critical)`;
            
            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.payload.pull_request.head.sha,
              state: state,
              target_url: `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              description: description,
              context: 'Mock Data Detection'
            });

  # ============================================================================
  # Security and Compliance Reporting
  # ============================================================================
  compliance-reporting:
    name: ðŸ“‹ Compliance & Security Reporting
    runs-on: ubuntu-latest
    needs: [preflight, mock-data-detection]
    if: always() && (github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'schedule')
    timeout-minutes: 10
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸ“Š Download All Detection Results
        uses: actions/download-artifact@v4
        with:
          pattern: mock-detection-results-*
          merge-multiple: true
      
      - name: ðŸ“ˆ Generate Compliance Dashboard
        run: |
          echo "ðŸ“Š Generating compliance dashboard..."
          
          python -c "
import json
import datetime
from pathlib import Path

# Aggregate all detection results
all_violations = []
total_files_scanned = 0
total_scan_time = 0

report_files = list(Path('.').glob('mock-detection-*.json'))
for report_file in report_files:
    with open(report_file) as f:
        data = json.load(f)
        all_violations.extend(data['detailed_violations'])
        total_files_scanned += data['scan_metadata']['files_scanned']
        total_scan_time += data['scan_metadata']['scan_time_seconds']

# Generate compliance report
compliance_report = {
    'timestamp': datetime.datetime.utcnow().isoformat(),
    'repository': '${{ github.repository }}',
    'branch': '${{ github.ref_name }}',
    'commit_sha': '${{ github.sha }}',
    'scan_summary': {
        'total_files_scanned': total_files_scanned,
        'total_violations': len(all_violations),
        'critical_violations': len([v for v in all_violations if v['severity'] == 'CRITICAL']),
        'high_violations': len([v for v in all_violations if v['severity'] == 'HIGH']),
        'scan_duration_seconds': total_scan_time,
        'compliance_status': 'COMPLIANT' if len([v for v in all_violations if v['severity'] in ['CRITICAL', 'HIGH']]) == 0 else 'NON_COMPLIANT'
    },
    'policy_enforcement': {
        'zero_tolerance_enabled': True,
        'enforcement_level': 'HIGH',
        'bypass_used': False,
        'audit_trail': True
    }
}

with open('compliance-dashboard.json', 'w') as f:
    json.dump(compliance_report, f, indent=2)

print('âœ… Compliance dashboard generated')
print(f'Status: {compliance_report[\"scan_summary\"][\"compliance_status\"]}')
print(f'Violations: {compliance_report[\"scan_summary\"][\"total_violations\"]}')
          "
      
      - name: ðŸ“¤ Upload Compliance Report
        uses: actions/upload-artifact@v4
        with:
          name: compliance-dashboard-${{ github.run_number }}
          path: compliance-dashboard.json
          retention-days: 90
      
      - name: ðŸš¨ Send Compliance Alerts
        if: contains(fromJson('["push", "schedule"]'), github.event_name)
        run: |
          echo "ðŸ”” Processing compliance alerts..."
          
          # Check if violations exist
          VIOLATIONS=$(python -c "
import json
with open('compliance-dashboard.json') as f:
    data = json.load(f)
    print(data['scan_summary']['total_violations'])
          ")
          
          if [[ $VIOLATIONS -gt 0 ]]; then
            echo "::warning title=Compliance Alert::$VIOLATIONS mock data violations detected in main branch"
            # In a real implementation, send alerts to security team
            echo "ðŸš¨ Compliance alert triggered - violations detected"
          else
            echo "âœ… Compliance check passed - no violations"
          fi

  # ============================================================================
  # Emergency Bypass Handling
  # ============================================================================
  emergency-bypass:
    name: ðŸš¨ Emergency Bypass Handler
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.skip-enforcement == 'true'
    timeout-minutes: 5
    
    steps:
      - name: ðŸ“ Log Emergency Bypass
        run: |
          echo "ðŸš¨ EMERGENCY BYPASS ACTIVATED"
          echo "Actor: ${{ github.actor }}"
          echo "Commit: ${{ github.sha }}"
          echo "Message: ${{ github.event.head_commit.message }}"
          echo "Timestamp: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
          
          echo "::warning title=Emergency Bypass::Mock data enforcement bypassed by authorized user"
      
      - name: ðŸ”” Notify Security Team
        run: |
          echo "ðŸ“§ Emergency bypass notification sent to security team"
          # In production, this would send alerts to security team
          echo "Security team has been notified of emergency bypass usage"

# ============================================================================
# Workflow Summary and Cleanup
# ============================================================================

  workflow-summary:
    name: ðŸ“Š Workflow Summary
    runs-on: ubuntu-latest
    needs: [preflight, mock-data-detection, pr-integration, compliance-reporting, emergency-bypass]
    if: always()
    timeout-minutes: 3
    
    steps:
      - name: ðŸ“‹ Generate Workflow Summary
        run: |
          echo "## ðŸš¨ Mock Data Enforcement Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job status summary
          echo "### Job Status" >> $GITHUB_STEP_SUMMARY
          echo "- Pre-flight: ${{ needs.preflight.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Mock Detection: ${{ needs.mock-data-detection.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- PR Integration: ${{ needs.pr-integration.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Compliance: ${{ needs.compliance-reporting.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Emergency Bypass: ${{ needs.emergency-bypass.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Detection results
          if [[ "${{ needs.mock-data-detection.outputs.violations-found }}" != "" ]]; then
            echo "### Detection Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Violations**: ${{ needs.mock-data-detection.outputs.violations-found }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Critical**: ${{ needs.mock-data-detection.outputs.critical-violations }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Final status
          OVERALL_STATUS="SUCCESS"
          if [[ "${{ needs.mock-data-detection.result }}" == "failure" ]]; then
            OVERALL_STATUS="FAILED"
          fi
          
          echo "### Overall Status: $OVERALL_STATUS" >> $GITHUB_STEP_SUMMARY
          
          if [[ "$OVERALL_STATUS" == "FAILED" ]]; then
            echo "ðŸš¨ Mock data violations detected - review and resolve before proceeding." >> $GITHUB_STEP_SUMMARY
          else
            echo "âœ… No mock data violations detected - all clear!" >> $GITHUB_STEP_SUMMARY
          fi
