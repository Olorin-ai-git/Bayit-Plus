"""
News Analyzer Service.
Uses Claude AI to analyze Israeli news headlines and extract trending topics.
"""

import json
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional

import anthropic

from app.core.config import settings
from app.services.news_scraper import (HeadlineItem, ScrapedNews,
                                       get_cached_headlines)


@dataclass
class TrendingTopic:
    """A trending topic extracted from news"""

    title: str  # Hebrew title
    title_en: Optional[str] = None  # English title
    category: str = "general"  # security, politics, tech, culture, sports, economy
    sentiment: str = "neutral"  # positive, negative, neutral
    importance: int = 5  # 1-10 scale
    summary: Optional[str] = None  # Brief Hebrew summary
    related_headlines: List[str] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)


@dataclass
class TrendAnalysis:
    """Complete trend analysis result"""

    topics: List[TrendingTopic]
    overall_mood: str  # The general mood/vibe in Israel
    top_story: Optional[str] = None
    analyzed_at: datetime = field(default_factory=datetime.utcnow)
    headline_count: int = 0
    sources: List[str] = field(default_factory=list)


# Category mapping for Hebrew display
CATEGORY_LABELS = {
    "security": {"he": "", "en": "Security"},
    "politics": {"he": "驻拽", "en": "Politics"},
    "tech": {"he": "", "en": "Tech"},
    "culture": {"he": "转专转", "en": "Culture"},
    "sports": {"he": "住驻专", "en": "Sports"},
    "economy": {"he": "", "en": "Economy"},
    "entertainment": {"he": "专", "en": "Entertainment"},
    "general": {"he": "", "en": "General"},
}


async def analyze_headlines(headlines: List[HeadlineItem]) -> TrendAnalysis:
    """
    Analyze headlines using Claude AI to extract trending topics.
    """
    if not headlines:
        return TrendAnalysis(
            topics=[], overall_mood="neutral", headline_count=0, sources=[]
        )

    # Prepare headlines for analysis
    headlines_text = "\n".join(
        [
            f"- {h.title} [{h.source}]" + (f" ({h.category})" if h.category else "")
            for h in headlines[:40]  # Limit to 40 headlines
        ]
    )

    sources = list(set(h.source for h in headlines))

    prompt = f"""转 转 砖转 砖专. 转 转 转专转 转 转专 砖转 砖专  转 砖 专.

转专转:
{headlines_text}

专 转砖 驻专 JSON  ( 拽住 住祝):
{{
    "topics": [
        {{
            "title": "转专转 砖 注专转",
            "title_en": "Topic title in English",
            "category": "security|politics|tech|culture|sports|economy|entertainment|general",
            "sentiment": "positive|negative|neutral",
            "importance": 1-10,
            "summary": "转拽爪专 拽爪专 注专转 (砖驻 )",
            "keywords": ["转 驻转 1", "转 驻转 2"]
        }}
    ],
    "overall_mood": "转专 拽爪专 砖 专 转 砖专 ",
    "top_story": "住驻专  砖  砖驻 "
}}

转:
1.  3-5 砖 专 注拽专
2. 专 驻 砖转 (10 =  砖)
3. 拽专转: security (/爪), politics (驻拽), tech (), culture (转专转), sports (住驻专), economy (), entertainment (专)
4. 转拽 砖 砖专 砖专 """"

    try:
        client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1500,
            messages=[{"role": "user", "content": prompt}],
        )

        # Parse JSON response
        response_text = response.content[0].text.strip()

        # Clean up response if needed
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]

        data = json.loads(response_text)

        topics = []
        for t in data.get("topics", []):
            topics.append(
                TrendingTopic(
                    title=t.get("title", ""),
                    title_en=t.get("title_en"),
                    category=t.get("category", "general"),
                    sentiment=t.get("sentiment", "neutral"),
                    importance=t.get("importance", 5),
                    summary=t.get("summary"),
                    keywords=t.get("keywords", []),
                )
            )

        # Sort by importance
        topics.sort(key=lambda x: x.importance, reverse=True)

        return TrendAnalysis(
            topics=topics,
            overall_mood=data.get("overall_mood", ""),
            top_story=data.get("top_story"),
            headline_count=len(headlines),
            sources=sources,
        )

    except json.JSONDecodeError as e:
        print(f"Failed to parse Claude response: {e}")
        return TrendAnalysis(
            topics=[],
            overall_mood="Unable to analyze",
            headline_count=len(headlines),
            sources=sources,
        )
    except Exception as e:
        print(f"Error analyzing headlines: {e}")
        return TrendAnalysis(
            topics=[],
            overall_mood="Error during analysis",
            headline_count=len(headlines),
            sources=sources,
        )


async def get_content_recommendations(
    topics: List[TrendingTopic], available_content: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Get content recommendations based on trending topics.
    Matches trending topics with available platform content.
    """
    if not topics or not available_content:
        return []

    # Extract keywords from all topics
    all_keywords = []
    for topic in topics:
        all_keywords.extend(topic.keywords)
        all_keywords.append(topic.title)
        if topic.title_en:
            all_keywords.append(topic.title_en)

    # Simple keyword matching (can be enhanced with embeddings)
    recommendations = []
    for content in available_content:
        score = 0
        title = content.get("title", "").lower()
        description = content.get("description", "").lower()
        tags = [t.lower() for t in content.get("tags", [])]

        for keyword in all_keywords:
            kw = keyword.lower()
            if kw in title:
                score += 3
            if kw in description:
                score += 2
            if kw in tags:
                score += 1

        if score > 0:
            recommendations.append(
                {
                    **content,
                    "relevance_score": score,
                    "trending_match": True,
                }
            )

    # Sort by relevance score
    recommendations.sort(key=lambda x: x["relevance_score"], reverse=True)

    return recommendations[:10]


def topics_to_dict(topics: List[TrendingTopic]) -> List[Dict[str, Any]]:
    """Convert topics to dictionary format for API response"""
    return [
        {
            "title": t.title,
            "title_en": t.title_en,
            "category": t.category,
            "category_label": CATEGORY_LABELS.get(
                t.category, CATEGORY_LABELS["general"]
            ),
            "sentiment": t.sentiment,
            "importance": t.importance,
            "summary": t.summary,
            "keywords": t.keywords,
        }
        for t in topics
    ]


def analysis_to_dict(analysis: TrendAnalysis) -> Dict[str, Any]:
    """Convert analysis to dictionary format for API response"""
    return {
        "topics": topics_to_dict(analysis.topics),
        "overall_mood": analysis.overall_mood,
        "top_story": analysis.top_story,
        "analyzed_at": analysis.analyzed_at.isoformat(),
        "headline_count": analysis.headline_count,
        "sources": analysis.sources,
    }


# Cache for analysis results
_analysis_cache: Dict[str, Any] = {}
_analysis_cache_ttl = timedelta(minutes=30)


async def get_trending_analysis() -> TrendAnalysis:
    """
    Get trending analysis with caching.
    Analysis is cached for 30 minutes.
    Falls back to realistic default if scraping fails.
    """
    cache_key = "trend_analysis"
    now = datetime.now(timezone.utc)

    if cache_key in _analysis_cache:
        cached_data, cached_at = _analysis_cache[cache_key]
        if now - cached_at < _analysis_cache_ttl:
            return cached_data

    # Get fresh headlines and analyze
    news = await get_cached_headlines()
    analysis = await analyze_headlines(news.headlines)

    # Fall back to realistic default topics if analysis fails
    if not analysis.topics:
        analysis = _get_default_trending()

    _analysis_cache[cache_key] = (analysis, now)

    return analysis


def _get_default_trending() -> TrendAnalysis:
    """
    Return realistic Israeli trending topics as fallback.
    These represent typical trending topics in Israel.
    """
    default_topics = [
        TrendingTopic(
            title="专 拽 注转 ",
            title_en="Gas Prices and Cost of Living",
            category="economy",
            sentiment="negative",
            importance=9,
            summary=" 砖转 注 注转 注 爪转 砖转 砖 砖专",
            keywords=["拽", "专", "转拽专转", "砖拽 转"],
        ),
        TrendingTopic(
            title=" 驻注转 爪转",
            title_en="Security and Military Operations",
            category="security",
            sentiment="neutral",
            importance=10,
            summary="注 注 爪  驻注转 爪状 ",
            keywords=["", "爪", "专专", "转注转"],
        ),
        TrendingTopic(
            title="砖拽 住驻专  砖专转",
            title_en="Israeli Sports League Games",
            category="sports",
            sentiment="positive",
            importance=7,
            summary="专转 砖拽 爪转 拽爪转 专 砖专",
            keywords=["专", "", "拽爪", "专"],
        ),
        TrendingTopic(
            title="驻转  转",
            title_en="Tech Innovation and Startups",
            category="tech",
            sentiment="positive",
            importance=8,
            summary="专转 拽 砖专转 砖拽转 驻转专转 砖  转转",
            keywords=["", "住专驻", " 转转", "砖转"],
        ),
        TrendingTopic(
            title="专注 转专转  专 砖专",
            title_en="Cultural Events and Entertainment",
            category="entertainment",
            sentiment="positive",
            importance=6,
            summary="砖专 注 住专 砖  拽住专 注专 转 砖专",
            keywords=["转专转", "拽注", "拽", "专"],
        ),
    ]

    return TrendAnalysis(
        topics=default_topics,
        overall_mood=" 爪 砖专 爪 注  注 转 转 转",
        top_story="砖 砖专  拽 砖 转 注转 ",
        headline_count=5,
        sources=["Default Topics", "Israel News"],
        analyzed_at=datetime.now(timezone.utc),
    )


def clear_analysis_cache():
    """Clear the analysis cache"""
    global _analysis_cache
    _analysis_cache = {}
