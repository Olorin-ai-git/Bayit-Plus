#!/usr/bin/env python3
"""
Simple test script for Vector Search Tool functionality.
This script tests the vector search tool directly without complex dependencies.
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime, timezone
from typing import Any, Dict, List

# Add the parent directory to the path so we can import from app
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Mock data for testing
MOCK_SPLUNK_RESULTS = [
    {
        "fuzzy_device_id": "device_001",
        "city": "San Francisco",
        "state": "CA",
        "country": "US",
        "tm_smart_id": "smart_123",
        "tm_true_ip_geo": "US",
        "tm_true_ip": "192.168.1.100",
        "tm_proxy_ip": None,
        "rss_epoch_time": "1640995200000",  # 2022-01-01 00:00:00
        "tm_os_anomaly": "false",
        "tm_http_os_signature": "windows 10",
        "tm_true_ip_longitude": "-122.4194",
        "tm_true_ip_latitude": "37.7749",
        "tm_input_ip_longitude": "-122.4194",
        "tm_input_ip_latitude": "37.7749",
        "tm_page_time_on": "30000",
        "tm_screen_color_depth": "32",
        "tm_agent_public_key_hash_type": "web:rsa",
        "tm_bb_bot_score": "100",
        "_time": "2022-01-01T00:00:00Z",
    },
    {
        "fuzzy_device_id": "device_002",
        "city": "San Francisco",
        "state": "CA",
        "country": "US",
        "tm_smart_id": "smart_123",  # Same smart ID - should be very similar
        "tm_true_ip_geo": "US",
        "tm_true_ip": "192.168.1.101",  # Different IP but same network
        "tm_proxy_ip": None,
        "rss_epoch_time": "1640995260000",  # 1 minute later
        "tm_os_anomaly": "false",
        "tm_http_os_signature": "windows 10",
        "tm_true_ip_longitude": "-122.4194",
        "tm_true_ip_latitude": "37.7749",
        "tm_input_ip_longitude": "-122.4194",
        "tm_input_ip_latitude": "37.7749",
        "tm_page_time_on": "35000",
        "tm_screen_color_depth": "32",
        "tm_agent_public_key_hash_type": "web:rsa",
        "tm_bb_bot_score": "120",
        "_time": "2022-01-01T00:01:00Z",
    },
    {
        "fuzzy_device_id": "device_003",
        "city": "New York",
        "state": "NY",
        "country": "US",
        "tm_smart_id": "smart_456",  # Different smart ID
        "tm_true_ip_geo": "US",
        "tm_true_ip": "10.0.0.50",
        "tm_proxy_ip": "proxy.suspicious.com",  # Using proxy - risk indicator
        "rss_epoch_time": "1640995320000",  # 2 minutes later
        "tm_os_anomaly": "true",  # OS anomaly - risk indicator
        "tm_http_os_signature": "linux ubuntu",
        "tm_true_ip_longitude": "-74.0060",
        "tm_true_ip_latitude": "40.7128",
        "tm_input_ip_longitude": "-74.0060",
        "tm_input_ip_latitude": "40.7128",
        "tm_page_time_on": "5000",
        "tm_screen_color_depth": "24",  # Suspicious color depth
        "tm_agent_public_key_hash_type": "web:ecdsa",  # Suspicious key type
        "tm_bb_bot_score": "800",  # High bot score - risk indicator
        "_time": "2022-01-01T00:02:00Z",
    },
    {
        "fuzzy_device_id": "device_004",
        "city": "Los Angeles",
        "state": "CA",
        "country": "US",
        "tm_smart_id": "smart_789",
        "tm_true_ip_geo": "US",
        "tm_true_ip": "172.16.0.25",
        "tm_proxy_ip": None,
        "rss_epoch_time": "1640995380000",  # 3 minutes later
        "tm_os_anomaly": "false",
        "tm_http_os_signature": "mac os x",
        "tm_true_ip_longitude": "-118.2437",
        "tm_true_ip_latitude": "34.0522",
        "tm_input_ip_longitude": "-118.2437",
        "tm_input_ip_latitude": "34.0522",
        "tm_page_time_on": "45000",
        "tm_screen_color_depth": "32",
        "tm_agent_public_key_hash_type": "web:rsa",
        "tm_bb_bot_score": "50",
        "_time": "2022-01-01T00:03:00Z",
    },
    {
        "fuzzy_device_id": "device_005",
        "city": "San Francisco",
        "state": "CA",
        "country": "US",
        "tm_smart_id": "smart_123",  # Same smart ID as first two - should be similar
        "tm_true_ip_geo": "US",
        "tm_true_ip": "192.168.1.102",
        "tm_proxy_ip": None,
        "rss_epoch_time": "1640995440000",  # 4 minutes later
        "tm_os_anomaly": "false",
        "tm_http_os_signature": "windows 10",
        "tm_true_ip_longitude": "-122.4194",
        "tm_true_ip_latitude": "37.7749",
        "tm_input_ip_longitude": "-122.4194",
        "tm_input_ip_latitude": "37.7749",
        "tm_page_time_on": "28000",
        "tm_screen_color_depth": "32",
        "tm_agent_public_key_hash_type": "web:rsa",
        "tm_bb_bot_score": "90",
        "_time": "2022-01-01T00:04:00Z",
    },
]


async def test_vector_search_tool_comprehensive():
    """Comprehensive test of the vector search tool with various scenarios."""

    from app.service.agent.tools.vector_search_tool import VectorSearchTool

    logger.info("=== COMPREHENSIVE VECTOR SEARCH TOOL TEST ===")

    tool = VectorSearchTool()

    # Test 1: Basic similarity search
    logger.info("--- Test 1: Basic Similarity Search ---")
    target_record = MOCK_SPLUNK_RESULTS[0]
    candidate_records = MOCK_SPLUNK_RESULTS[1:]

    result = await tool._arun(
        target_record=target_record,
        candidate_records=candidate_records,
        max_results=5,
        distance_threshold=15.0,
    )

    logger.info(
        f"Test 1 Result: Found {len(result['similar_records'])} similar records"
    )

    # Verify structure
    assert "target_record" in result
    assert "similar_records" in result
    assert "total_candidates" in result
    assert "total_results" in result

    similar_records = result["similar_records"]

    # Check that records are sorted by distance
    if len(similar_records) > 1:
        distances = [record["distance"] for record in similar_records]
        assert distances == sorted(distances), "Records should be sorted by distance"
        logger.info("âœ… Records are properly sorted by distance")

    # Log distance details
    for i, record in enumerate(similar_records):
        logger.info(
            f"Record {i+1}: distance={record['distance']:.2f}, smart_id={record['record'].get('tm_smart_id')}"
        )

    # Test 2: Strict distance threshold
    logger.info("--- Test 2: Strict Distance Threshold ---")
    strict_result = await tool._arun(
        target_record=target_record,
        candidate_records=candidate_records,
        max_results=10,
        distance_threshold=2.0,  # Very strict threshold
    )

    logger.info(
        f"Test 2 Result: Found {len(strict_result['similar_records'])} records with strict threshold"
    )

    # Should find only very similar records (same smart_id)
    for record in strict_result["similar_records"]:
        assert (
            record["distance"] <= 2.0
        ), f"Record distance {record['distance']} exceeds threshold"
        logger.info(
            f"Strict match: distance={record['distance']:.2f}, smart_id={record['record'].get('tm_smart_id')}"
        )

    # Test 3: Max results limit
    logger.info("--- Test 3: Max Results Limit ---")
    limited_result = await tool._arun(
        target_record=target_record,
        candidate_records=candidate_records,
        max_results=2,  # Limit to 2 results
        distance_threshold=None,
    )

    logger.info(
        f"Test 3 Result: Found {len(limited_result['similar_records'])} records with max_results=2"
    )
    assert (
        len(limited_result["similar_records"]) <= 2
    ), "Should respect max_results limit"

    # Test 4: Risk pattern detection
    logger.info("--- Test 4: Risk Pattern Detection ---")
    risky_target = MOCK_SPLUNK_RESULTS[
        2
    ]  # The suspicious record with proxy, high bot score, etc.

    risk_result = await tool._arun(
        target_record=risky_target,
        candidate_records=[r for r in MOCK_SPLUNK_RESULTS if r != risky_target],
        max_results=5,
        distance_threshold=None,
    )

    logger.info(f"Test 4 Result: Risk analysis for suspicious record")
    logger.info(
        f"Target has: proxy={risky_target.get('tm_proxy_ip')}, bot_score={risky_target.get('tm_bb_bot_score')}"
    )

    for i, record in enumerate(risk_result["similar_records"]):
        rec = record["record"]
        logger.info(
            f"Similar record {i+1}: distance={record['distance']:.2f}, "
            f"proxy={rec.get('tm_proxy_ip')}, bot_score={rec.get('tm_bb_bot_score')}"
        )

    # Test 5: Geographic similarity
    logger.info("--- Test 5: Geographic Similarity ---")
    sf_records = [r for r in MOCK_SPLUNK_RESULTS if r.get("city") == "San Francisco"]
    non_sf_records = [
        r for r in MOCK_SPLUNK_RESULTS if r.get("city") != "San Francisco"
    ]

    geo_result = await tool._arun(
        target_record=sf_records[0],
        candidate_records=sf_records[1:] + non_sf_records,
        max_results=10,
        distance_threshold=None,
    )

    logger.info(f"Test 5 Result: Geographic similarity analysis")
    for i, record in enumerate(geo_result["similar_records"]):
        rec = record["record"]
        logger.info(
            f"Record {i+1}: distance={record['distance']:.2f}, "
            f"city={rec.get('city')}, smart_id={rec.get('tm_smart_id')}"
        )

    logger.info("âœ… ALL COMPREHENSIVE TESTS PASSED!")
    return result


async def test_pattern_analysis():
    """Test pattern analysis functionality that would be used in the client."""

    logger.info("=== PATTERN ANALYSIS TEST ===")

    from app.service.agent.tools.vector_search_tool import VectorSearchTool

    tool = VectorSearchTool()

    # Simulate the analysis that would happen in the client
    target_record = MOCK_SPLUNK_RESULTS[0]
    candidate_records = MOCK_SPLUNK_RESULTS[1:]

    search_result = await tool._arun(
        target_record=target_record,
        candidate_records=candidate_records,
        max_results=10,
        distance_threshold=10.0,
    )

    # Analyze patterns (similar to what the client does)
    similar_records = search_result.get("similar_records", [])

    if not similar_records:
        logger.info("No similar records found for pattern analysis")
        return

    # Analyze distance distribution
    distances = [record["distance"] for record in similar_records]

    # Count records by similarity level
    very_similar = len([d for d in distances if d <= 2.0])
    moderately_similar = len([d for d in distances if 2.0 < d <= 5.0])
    somewhat_similar = len([d for d in distances if 5.0 < d <= 10.0])

    logger.info(f"Similarity Distribution:")
    logger.info(f"  Very similar (â‰¤2.0): {very_similar}")
    logger.info(f"  Moderately similar (2.0-5.0): {moderately_similar}")
    logger.info(f"  Somewhat similar (5.0-10.0): {somewhat_similar}")
    logger.info(f"  Total: {len(similar_records)}")

    # Analyze common features
    records = [record["record"] for record in similar_records]
    key_fields = [
        "tm_smart_id",
        "tm_true_ip_geo",
        "tm_true_ip",
        "tm_http_os_signature",
        "tm_os_anomaly",
    ]

    logger.info(f"Common Features Analysis:")
    for field in key_fields:
        values = [record.get(field) for record in records if record.get(field)]
        if values:
            value_counts = {}
            for value in values:
                value_counts[value] = value_counts.get(value, 0) + 1

            most_common = (
                max(value_counts.items(), key=lambda x: x[1]) if value_counts else None
            )
            if most_common:
                logger.info(
                    f"  {field}: '{most_common[0]}' appears {most_common[1]}/{len(values)} times"
                )

    # Assess risk indicators
    risk_indicators = {
        "proxy_usage": 0,
        "suspicious_bot_scores": 0,
        "os_anomalies": 0,
        "suspicious_color_depth": 0,
        "total_records": len(records),
    }

    for record in records:
        if record.get("tm_proxy_ip"):
            risk_indicators["proxy_usage"] += 1

        bot_score = record.get("tm_bb_bot_score")
        if bot_score and float(bot_score) > 500:
            risk_indicators["suspicious_bot_scores"] += 1

        if record.get("tm_os_anomaly") == "true":
            risk_indicators["os_anomalies"] += 1

        if record.get("tm_screen_color_depth") == "24":
            risk_indicators["suspicious_color_depth"] += 1

    logger.info(f"Risk Indicators:")
    total = risk_indicators["total_records"]
    for key, count in risk_indicators.items():
        if key != "total_records":
            percentage = (count / total * 100) if total > 0 else 0
            logger.info(f"  {key}: {count}/{total} ({percentage:.1f}%)")

    logger.info("âœ… PATTERN ANALYSIS TEST PASSED!")


async def main():
    """Run all tests."""
    logger.info("Starting Vector Search Tool Tests")

    try:
        # Test vector search tool comprehensively
        await test_vector_search_tool_comprehensive()

        # Test pattern analysis
        await test_pattern_analysis()

        logger.info("ðŸŽ‰ ALL TESTS COMPLETED SUCCESSFULLY!")

        # Summary
        logger.info("\n=== INTEGRATION SUMMARY ===")
        logger.info("âœ… Vector Search Tool is working correctly")
        logger.info("âœ… Distance function handles various data types properly")
        logger.info("âœ… Similarity detection works for same smart_id records")
        logger.info("âœ… Risk pattern detection identifies suspicious indicators")
        logger.info("âœ… Geographic and behavioral similarity analysis functional")
        logger.info("âœ… Pattern analysis provides meaningful insights")
        logger.info(
            "\nThe vector search tool is ready for integration with the location data client!"
        )

    except Exception as e:
        logger.error(f"ðŸ’¥ TESTS FAILED: {str(e)}", exc_info=True)
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
